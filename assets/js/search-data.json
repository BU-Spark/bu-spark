{"0": {
    "doc": "Making Sense of the Census",
    "title": "Making Sense of the Census üè° üìä",
    "content": "October 25, 2023 . These slides were modified from Max Palmer. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#making-sense-of-the-census--",
    "relUrl": "/documentation/project-guides/census_tutorial.html#making-sense-of-the-census--"
  },"1": {
    "doc": "Making Sense of the Census",
    "title": "Agenda",
    "content": ". | What is the Census? What is the Census Bureau? | What is in the Decennial Census and ACS? | Where can I find Census Data? | Census Geography | Race and Ethnicity in the Census | Census API Tutorial | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#agenda",
    "relUrl": "/documentation/project-guides/census_tutorial.html#agenda"
  },"2": {
    "doc": "Making Sense of the Census",
    "title": "What is the Census? What is the Census Bureau?",
    "content": ". | Census Bureau is an ‚Äúagency‚Äù ‚Äî the part of the federal government responsible for collecting data about the American people and the economy. | Decennial Census | The American Community Survey (ACS) | Other Surveys/Programs | Established by the US Constitution | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#what-is-the-census-what-is-the-census-bureau",
    "relUrl": "/documentation/project-guides/census_tutorial.html#what-is-the-census-what-is-the-census-bureau"
  },"3": {
    "doc": "Making Sense of the Census",
    "title": "What is in the Decennial Census and ACS?",
    "content": ". | Decennial Census (collected every 10 years) . | Sex; age; Hispanic origin; race | Main Function: provide counts of people for the purpose of congressional apportionment | . | American Community Survey (collected every year) . | more data than decennial | 1 year, 5 year | Main Function: measure the changing social and economic characteristics of our population (in terms of education, housing, jobs, and more) | . | Estimations | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#what-is-in-the-decennial-census-and-acs",
    "relUrl": "/documentation/project-guides/census_tutorial.html#what-is-in-the-decennial-census-and-acs"
  },"4": {
    "doc": "Making Sense of the Census",
    "title": "Where can I find Census Data?",
    "content": ". | Census Website | Census API | National Historical Geographic Information System (NHGIS) | Integrated Public Use Microdata Sample (IPUMS) | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#where-can-i-find-census-data",
    "relUrl": "/documentation/project-guides/census_tutorial.html#where-can-i-find-census-data"
  },"5": {
    "doc": "Making Sense of the Census",
    "title": "Census Geography",
    "content": ". | Main geography for census is TIGER/Line. Set of shapefiles/geodatabases. Census also produces Cartographic Boundary files. | Good for mapping but not detailed geographic analysis. | . | Census geography is organized around the census block . | Blocks cover entire area of United States | Blocks do not overlap | Nested within block groups, tracts, counties, and states | Legislative districts assembled from blocks | . | Other geographies may not align perfectly with blocks/census geography | . . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#census-geography",
    "relUrl": "/documentation/project-guides/census_tutorial.html#census-geography"
  },"6": {
    "doc": "Making Sense of the Census",
    "title": "Census Geography",
    "content": ". | Blocks -&gt; Block Groups -&gt; Tracts -&gt; Counties -&gt; States . | These are perfectly nested, no overlaps | Blocks identified by GEOID | . | CCDS: 250250101033011 . | 25############# - state (State FIPS Code) | ##025########## - county (County FIPS Code; this can be combined with state, 25025) | #####010103#### - tract (Note: tract is often named as 101.03) | ###########3### - block group | ############011 - block (Note: blocks usually include block group, 3011) | . | Data: . | Decennial data: available by block | ACS data: available by block group | . | There are other census geographies available other than the ones we mentioned | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#census-geography-1",
    "relUrl": "/documentation/project-guides/census_tutorial.html#census-geography-1"
  },"7": {
    "doc": "Making Sense of the Census",
    "title": "Census Geography",
    "content": ". | Blocks are bounded by both visible features and nonvisible boundaries . | Visible Features: streets, roads, streams, railroad tracks | Nonvisible Boundaries: property lines and city, township, school district, and county limits | . | Blocks are not restricted by population, some blocks don‚Äôt have any people | Block Groups are statistical divisions of census tracts . | Generally defined to contain 600 and 3,000 people | A block group usually covers a contiguous area | Each census tract contains at least one block group and they are uniquely numbered within a census tract | . | Census Tracts generally have a population size between 1,200 and 8,000 people with an optimum size of 4,000 people. | Covers a contiguous area | Spatial size of census tracts varies depending on the denisty of settlement | . | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#census-geography-2",
    "relUrl": "/documentation/project-guides/census_tutorial.html#census-geography-2"
  },"8": {
    "doc": "Making Sense of the Census",
    "title": "Race and Ethnicity in the Census",
    "content": ". | The census asks about race and ethnicity separately | 2020 census collects data on Hispanic origin and race in 2 separate questions | The census follows standards on race and ethnicity set by Office of Management and Budget. The standards identify five minimum categories: . | White | Black or African American | American Indian or Alaska Native | Asian | Native Hawaiian or Other Pacific Islander | PLUS a sixth category ‚ÄúSome other Race‚Äù for people who don‚Äôt identify with any of the OMB race categories. | . | For ethnicity, the OMB standards classify individuals in one of two categories: . | Hispanic or Latinx or Not Hispanic or Latinx | . | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#race-and-ethnicity-in-the-census",
    "relUrl": "/documentation/project-guides/census_tutorial.html#race-and-ethnicity-in-the-census"
  },"9": {
    "doc": "Making Sense of the Census",
    "title": "Working with Census Data Programmatically",
    "content": ". | Census has a great API | R packages: tigris (for shapefiles); tidycensus for data | Python: CensusData | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#working-with-census-data-programmatically",
    "relUrl": "/documentation/project-guides/census_tutorial.html#working-with-census-data-programmatically"
  },"10": {
    "doc": "Making Sense of the Census",
    "title": "Using the Census API",
    "content": "Here are all of the publicly available datasets offered by the Census Bureau. We will go over calling data from the Decennial Census. Reference this documentation to gain a better understanding of the API and how to use it. import requests . # set variables year='2020' dsource='dec' # which survey are we interested in? -&gt; decennial dname='pl' # a dataset within a survey, pl -&gt; redistricting data state='25' # state code -&gt; Massachusetts . Below we specify the specific variables (data columns) we want. Here is the complete list of available variables. | P2_001N - Total | P2_002N - Total: Hispanic or Latino | P2_003N - Total: Not Hispanic or Latino | P2_004N - Total: Not Hispanic or Latino: Population of One Race | P2_005N - Total: Not Hispanic or Latino: Population of One Race: White alone | P2_006N - Total: Not Hispanic or Latino: Population of One Race: Black or African American alone | P2_007N - Total: Not Hispanic or Latino: Population of One Race: American Indian and Alaska Native alone | P2_008N - Total: Not Hispanic or Latino: Population of One Race: Asian alone | P2_009N - Total: Not Hispanic or Latino: Population of One Race: Native Hawaiian and Other Pacific Islander alone | P2_010N - Total: Not Hispanic or Latino: Population of One Race: Some Other Race Alone | . # specify the specific data columns we want cols = 'NAME,P2_001N,P2_002N,P2_003N,P2_004N,P2_005N,\\ P2_006N,P2_007N,P2_008N,P2_009N,P2_010N' . Call the API to get demographic data for Massachusetts. base_url = f\"https://api.census.gov/data/{year}/{dsource}/{dname}\" census_url = f\"{base_url}?get={cols}&amp;for=state:{state}\" census_response = requests.get(census_url) # check if the response was successful if census_response.status_code == 200: data = census_response.json() print(data[0]) # print column names print(data[1]) # print data else: # response was not successful print('Error: ' + str(census_response.status_code)) . ['NAME', 'P2_001N', 'P2_002N', 'P2_003N', 'P2_004N', 'P2_005N', 'P2_006N', 'P2_007N', 'P2_008N', 'P2_009N', 'P2_010N', 'state'] ['Massachusetts', '7029917', '887685', '6142232', '5813954', '4748897', '457055', '9387', '504900', '1607', '92108', '25'] . Within the Redistricting Data we can get data down to the block level. Not all Census Data Products will allow you to get data down to the block level. You will need to read the documentation to figure out what data is available. Here is more information on finding the right Census Data Product to use. # get block demographics for block 3011, census block that ccds is in county = '025' tract = '010103' block = '3011' block_url = f\"{base_url}?get={cols}&amp;for=block:{block}&amp;in=tract:{tract}\\ &amp;in=county:{county}&amp;in=state:{state}\" block_response = requests.get(block_url) block_data = block_response.json() print(block_data[0]) # print column names print(block_data[1]) # print data . ['NAME', 'P2_001N', 'P2_002N', 'P2_003N', 'P2_004N', 'P2_005N', 'P2_006N', 'P2_007N', 'P2_008N', 'P2_009N', 'P2_010N', 'state', 'county', 'tract', 'block'] ['Block 3011, Block Group 3, Census Tract 101.03, Suffolk County, Massachusetts', '71', '9', '62', '61', '41', '6', '0', '10', '0', '4', '25', '025', '010103', '3011'] . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#using-the-census-api",
    "relUrl": "/documentation/project-guides/census_tutorial.html#using-the-census-api"
  },"11": {
    "doc": "Making Sense of the Census",
    "title": "Other Resources",
    "content": ". | Analyze Boston has census data for Boston at tract level and block group level . | Plus Boston Neighborhoods! | . | Census Geocoder | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html#other-resources",
    "relUrl": "/documentation/project-guides/census_tutorial.html#other-resources"
  },"12": {
    "doc": "Making Sense of the Census",
    "title": "Making Sense of the Census",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-guides/census_tutorial.html",
    "relUrl": "/documentation/project-guides/census_tutorial.html"
  },"13": {
    "doc": "Data Science Tech Stack",
    "title": "Data Science Tech Stack",
    "content": "By the end of this document, you will have gained an idea of the common technologies that data scientists use. | Data Science Tech Stack . | Microsoft Excel and Google Sheets | Getting Started . | Anaconda | Google Colab | . | Python . | Numpy | Pandas | Matplotlib | Seaborn/Plotly | Scikit-learn | . | SQL | Kaggle and Google Dataset Search | . | . ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/data-science.html",
    "relUrl": "/documentation/tech-stack/data-science.html"
  },"14": {
    "doc": "Data Science Tech Stack",
    "title": "Microsoft Excel and Google Sheets",
    "content": "Data science is a broad field that involves collecting, cleaning, analyzing, and interpreting data to extract insights and make informed decisions. While coding is an essential aspect of data science, not all data science problems require programming skills. In some cases, simple calculations or analyses can be done using tools like Microsoft Excel or Google Sheets. For instance, you can use Excel and Sheets to calculate basic statistical measures like mean, median, mode, and standard deviation and creating visualizations. Simple data wrangling is also possible‚Äîthere are ways to remove nulls/duplicates and do feature engineering. Additionally, altering specific values in a table is easier in these technologies than in code. Excel and Google Sheets can also be used to perform simple predictive analyses. For instance, you can use regression analysis to model the relationship between two variables and make predictions about the future based on the model. While this may not be as sophisticated as other coding alternatives, it can be useful in making simple predictions based on historical data. Here is an article that goes through how you can do data analysis with Google Sheets and here is one for Excel. ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/data-science.html#microsoft-excel-and-google-sheets",
    "relUrl": "/documentation/tech-stack/data-science.html#microsoft-excel-and-google-sheets"
  },"15": {
    "doc": "Data Science Tech Stack",
    "title": "Getting Started",
    "content": "Before writing any code, we need to choose a place to write the code. Integrated Development Environments (IDEs) are software applications that provide a comprehensive environment for developing and testing code. IDEs typically offer features such as code debugging, syntax highlighting, auto-completion, and version control, making them an ideal choice for software developers who need to work on large-scale projects. Anaconda . A Jupyter Notebook is an open-source web application that allows users to create and share documents that contain live code, equations, visualizations, and narrative text. This tool is particularly useful for data scientists who need to combine code, data, and explanatory text in a single document. Jupyter Notebook supports multiple programming languages such as Python, R, and Julia, making it a versatile tool for data analysis. An easy way to get set up a Jupyter Notebook is with Anaconda. Anaconda can be thought of as a suite of useful programs for writing code in Python and R; it comes with a large number of fundamental data science libraries and tools and provides a convenient way to install, manage, and update all the other essential libraries and dependencies that data scientists use. Google Colab . Another option is Google Colab. If you want to code in Python but don‚Äôt want to install anything, then Google Colab is for you. Some benefits over Anaconda include: notebook sharing at the click of a button, access to Google‚Äôs hardware resources for faster model training, and Git/Github integration. There‚Äôs not much setup needed, just make sure you‚Äôre logged into your Google account and start coding! . ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/data-science.html#getting-started",
    "relUrl": "/documentation/tech-stack/data-science.html#getting-started"
  },"16": {
    "doc": "Data Science Tech Stack",
    "title": "Python",
    "content": "Numpy . NumPy is a Python library that provides support for large, multi-dimensional arrays and matrices, as well as a variety of mathematical functions to operate on these arrays. It is widely used in data science for data manipulation, numerical analysis, and scientific computing. NumPy provides a powerful and efficient way to work with large datasets, especially when they involve mathematical operations or linear algebra. NumPy is particularly useful for machine learning, as it provides a way to perform linear algebra operations on large matrices quickly and efficiently. Some key functions of NumPy: . | easy linear algebra operations | preprocessing data (normalizing, scaling) | calculating statistics (faster, more built in functions, integration with other libraries) | . Pandas . Pandas is a popular open-source Python library and will be your best friend for data manipulation and analysis. It provides a fast, flexible, and efficient data structure for working with structured data. Pandas is widely used by data scientists for a number range of tasks, including data cleaning, data transformation, and data analysis. In fact, Pandas is built on top of NumPy, which makes for fast and efficient numerical operations. In Pandas, a DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of them like a spreadsheet‚Äìthey have rows and columns and values in each cell. By using Pandas to write code, we can have complete control over a dataframe and perform many granular tasks that are not possible in applications like Google Sheets or Excel. A few basic Pandas functions: . | Easy Data Manipulation: DataFrames provide a convenient and efficient way to manipulate data, such as filtering, merging, grouping, and sorting. Pandas provides a wide range of functions to handle these operations, making data manipulation tasks faster and more efficient. | Missing Data Handling: DataFrames can handle missing data values, which is a common issue in real-world datasets. Pandas provides several functions to detect, remove, or fill missing values, making it easier to clean and preprocess data. | Data Visualization: Pandas provides integration with several visualization libraries, such as Matplotlib and Seaborn, to visualize and explore data in the DataFrame. This allows data scientists to quickly identify patterns and relationships in the data. | Performance: Pandas is built on top of NumPy, which provides fast and efficient computations on arrays. This allows DataFrames to handle large datasets and perform complex operations quickly and efficiently. | Integration: Pandas integrates well with other data science tools and libraries, such as scikit-learn, TensorFlow, and PyTorch. This makes it easier to integrate DataFrames into the broader data analysis and machine learning workflow. | . Check out this article to see some common functions. Here is a useful cheatsheet with a lot more functions! . Matplotlib . Matplotlib is a Python package used by data scientists to create quick, on-the-go visualizations of their data. Matplotlib provides a range of customizable plots, including line plots, scatter plots, bar plots, and histograms, among others. It also offers a range of customization options, allowing users to adjust everything from axis labels to colors and fonts. It integrates well with other popular data science packages in Python, such as NumPy and Pandas. This allows them to easily create visualizations of their data when doing EDA. Check out this beginner-friendly article about matplotlib and a figure‚Äôs anatomy. Seaborn/Plotly . Seaborn and Plotly are two more powerful visualization tools used in data science to create more ‚Äúfinal-product-y‚Äù and interactive graphs and charts. While Matplotlib is a great base plotting library, Seaborn and Plotly take it to the next level by providing more advanced features and improved aesthetics. Seaborn is known for its ability to create aesthetically pleasing static visualizations with minimal coding, making it a popular choice for creating dashboards after doing the actual analysis and etc. It provides default color palettes and styles that can be easily customized to fit the specific needs of the user, and overall a more polished finish to the plots it produces. Another popular option for more ‚Äúfinal-product-y‚Äù graphs is Plotly, a library that allows users to create interactive web-based visualizations in Python. Plotly‚Äôs interactive features are its main selling point, they allow users to zoom, pan, and hover over data points to see their values. This can be especially useful when dealing with large datasets, as it allows users to explore the data in more detail. Plotly also allows users to create 3D visualizations and animations, which can be useful in certain applications. Scikit-learn . Scikit-learn, also known as sklearn, is a machine learning library for Python. It is built on top of NumPy, SciPy, and matplotlib, and provides a wide range of tools for different tasks related to machine learning, such as classification, regression, clustering, dimensionality reduction, model selection, and preprocessing. Just to name some, linear and logistic regression, support vector machines, decision trees, random forests, and k-means clustering are all included in sklearn and can be implemented in just several lines of code. In addition to the models, scikit-learn offers a variety of methods and tools for data preprocessing, model selection, and pipelines. For instance, data normalization, scaling, encoding categorical features, imputing missing values, and feature selection are among the preprocessing tools available. Model selection and hyperparameter tuning can be performed using cross-validation, grid search, and randomized search methods. Additionally, the pipeline class enables the chaining of multiple preprocessing and modeling steps into a single object. ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/data-science.html#python",
    "relUrl": "/documentation/tech-stack/data-science.html#python"
  },"17": {
    "doc": "Data Science Tech Stack",
    "title": "SQL",
    "content": "Structured Query Language (SQL) is a powerful and widely used programming language for managing relational databases. SQL allows users to create, modify, and retrieve data from large databases. It is important because sometimes you will not just be given a csv file with all the correct data to analyze, but you will be given access to a database and will have to produce the data yourself. SQL allows data scientists to query data sets, filter data, and join tables to create new data sets. SQL also allows data scientists to aggregate data and calculate summary statistics‚Äìsounds a little like Pandas right? So what‚Äôs the difference? . SQL is designed to work with large databases, and it can handle millions of records and terabytes of data. Handling this amount of data with Pandas, would be slow and troublesome. However, SQL can be limited when it comes to data cleaning and transformation as it does not have built-in functions for data cleaning or transformation like Pandas does. In addition, SQL is not ideal for complex statistical analysis or machine learning. ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/data-science.html#sql",
    "relUrl": "/documentation/tech-stack/data-science.html#sql"
  },"18": {
    "doc": "Data Science Tech Stack",
    "title": "Kaggle and Google Dataset Search",
    "content": "Kaggle is a platform for data scientists to find and participate in machine learning competitions, collaborate on data science projects, and learn from other data scientists. The most appealing part of Kaggle is that it hosts a large collection of datasets that can be used for research and analysis. These datasets cover a wide range of topics, from healthcare, finance, and social sciences to video games, zebra crossings, and mushroom attributes. The idea is that this library gives you a playground to practice your skills in. Google Dataset Search is an alternative, it looks for datasets across the entire internet (including kaggle), so if you‚Äôre looking for something niche then GDS may be worth looking at. It is primarily a search engine for datasets, while Kaggle is more of a community and platform for data scientists that includes a bunch of downloadable datasets. ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/data-science.html#kaggle-and-google-dataset-search",
    "relUrl": "/documentation/tech-stack/data-science.html#kaggle-and-google-dataset-search"
  },"19": {
    "doc": "Getting Started With Exploratory Data Analysis (EDA)",
    "title": "Getting Started With Exploratory Data Analysis (EDA)",
    "content": "This notebook serves as a starter guide or template for exploratory data analysis. It will go over the topics mentioned in the EDA guide. # let's start off by importing the libraries we will need for eda import pandas as pd import numpy as np # for visualizations : import seaborn as sns import matplotlib.pyplot as plt . The dataset we will be using in this tutorial is from Analyze Boston. Analyze Boston is the City of Boston‚Äôs data hub and is a great resource for data sets regarding the city. We will be working with the 2022 311 Service Requests dataset. The dataset consists of service requests from all channels of engagement. 311 allows you to report non-emergency issues or request non-emergency City services. Link to dataset . # to run in colab, run the following lines # from google.colab import drive # drive.mount('/content/drive') . Mounted at /content/drive . # read in dataset df = pd.read_csv('311-requests.csv') pd.set_option('display.max_columns', 6) . # let's look at the first five rows of the dataset df.head() . | | case_enquiry_id | open_dt | target_dt | ... | latitude | longitude | source | . | 0 | 101004116078 | 2022-01-04 15:34:00 | NaN | ... | 42.3818 | -71.0322 | Citizens Connect App | . | 1 | 101004113538 | 2022-01-01 13:40:13 | 2022-01-04 08:30:00 | ... | 42.3376 | -71.0774 | City Worker App | . | 2 | 101004120888 | 2022-01-09 12:40:43 | 2022-01-11 08:30:00 | ... | 42.3431 | -71.0683 | City Worker App | . | 3 | 101004120982 | 2022-01-09 13:56:00 | NaN | ... | 42.3810 | -71.0256 | Constituent Call | . | 4 | 101004127209 | 2022-01-15 20:42:00 | 2022-01-20 08:30:00 | ... | 42.3266 | -71.0704 | Constituent Call | . 5 rows √ó 29 columns . How many observations/rows are there? . How many variables/columns are there? . What kinds of variables are there? Qualitative? Quantitative? Both? . # number of observations df.shape[0] . 146373 . # to see column name, count, and dtype of each column df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 146373 entries, 0 to 146372 Data columns (total 29 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 case_enquiry_id 146373 non-null int64 1 open_dt 146373 non-null object 2 target_dt 129475 non-null object 3 closed_dt 125848 non-null object 4 ontime 146373 non-null object 5 case_status 146373 non-null object 6 closure_reason 146373 non-null object 7 case_title 146371 non-null object 8 subject 146373 non-null object 9 reason 146373 non-null object 10 type 146373 non-null object 11 queue 146373 non-null object 12 department 146373 non-null object 13 submittedphoto 55740 non-null object 14 closedphoto 0 non-null float64 15 location 146373 non-null object 16 fire_district 146149 non-null object 17 pwd_district 146306 non-null object 18 city_council_district 146358 non-null object 19 police_district 146306 non-null object 20 neighborhood 146210 non-null object 21 neighborhood_services_district 146358 non-null object 22 ward 146373 non-null object 23 precinct 146270 non-null object 24 location_street_name 145030 non-null object 25 location_zipcode 110519 non-null float64 26 latitude 146373 non-null float64 27 longitude 146373 non-null float64 28 source 146373 non-null object dtypes: float64(4), int64(1), object(24) memory usage: 32.4+ MB . There are 146373 rows (observations). There are 29 columns (variables). There are both categorical and numerical variables. At quick glance there seems to be more categorical variables than numerical variables. Categorical Variables: case_status, neighborhood, source, etc. Numerical Variables: ‚Ä¶ maybe not? . The case_enquiry_id is a unique identifier for each row, closedphoto has 0 non-null values so it might be worth it to drop this column since there is no additional information we can gather, columns such as location_zipcode, latitude, longitude not exactly numeric varaibles, since they are numbers that represent different codes. Cleaning . Let‚Äôs convert the three time variables (open_dt, target_dt, and closed_dt) from objects to pandas datetime objects. Let‚Äôs focus on service requests for a set period of time in 2022. We will start by filtering for service requests that were opened from January 2022 to March 2022. # changing the three columns with dates and times to pandas datetime object df['open_dt'] = pd.to_datetime(df['open_dt']) df['target_dt'] = pd.to_datetime(df['target_dt']) df['closed_dt'] = pd.to_datetime(df['closed_dt']) # output is long, but run the line below to check the type of the three columns #df.dtypes . # filter data for 311 requests from january 2022 to march 2022 df_filtered = df.loc[(df['open_dt'] &gt;= '2022-01-01') &amp; (df['open_dt'] &lt; '2022-03-31')] df_filtered.head() . | | case_enquiry_id | open_dt | target_dt | ... | latitude | longitude | source | . | 0 | 101004116078 | 2022-01-04 15:34:00 | NaT | ... | 42.3818 | -71.0322 | Citizens Connect App | . | 1 | 101004113538 | 2022-01-01 13:40:13 | 2022-01-04 08:30:00 | ... | 42.3376 | -71.0774 | City Worker App | . | 2 | 101004120888 | 2022-01-09 12:40:43 | 2022-01-11 08:30:00 | ... | 42.3431 | -71.0683 | City Worker App | . | 3 | 101004120982 | 2022-01-09 13:56:00 | NaT | ... | 42.3810 | -71.0256 | Constituent Call | . | 4 | 101004127209 | 2022-01-15 20:42:00 | 2022-01-20 08:30:00 | ... | 42.3266 | -71.0704 | Constituent Call | . 5 rows √ó 29 columns . From our previous observation, since closedphoto column does not contain any non-null values, let‚Äôs drop it. # drop closedphoto column df_filtered = df_filtered.drop(columns=['closedphoto']) . After filtering the service requests, let‚Äôs see how many observations we are left with. # how many requests were opened from Jan 2022 to March 2022 df_filtered.shape[0] . 66520 . From a quick preview of the dataframe, we can see that some of the requests are still open. Let‚Äôs see how many observations are open vs. closed and then how many are ontime vs. overdue from the set of requests from January 2022 to March 2022. # checking how many open vs. closed cases df_filtered['case_status'].value_counts() . Closed 59420 Open 7100 Name: case_status, dtype: int64 . # visualize case_status in pie chart, set color palette colors = sns.color_palette('muted')[0:5] ax = df_filtered['case_status'].value_counts().plot.pie(colors=colors) ax.yaxis.set_visible(False) . # checking how many ontime vs. overdue cases df_filtered['ontime'].value_counts() . ONTIME 55089 OVERDUE 11431 Name: ontime, dtype: int64 . # visualize ontime in pie chart, set color palette colors = sns.color_palette('bright')[0:5] ax = df_filtered['ontime'].value_counts().plot.pie(colors=colors) ax.yaxis.set_visible(False) . Descriptive Statistics . Pandas makes this easy! We can use describe() to get the descriptive statistics of the numerical columns. df_filtered.describe() . | | case_enquiry_id | location_zipcode | latitude | longitude | . | count | 6.652000e+04 | 49807.000000 | 66520.000000 | 66520.000000 | . | mean | 1.010042e+11 | 2126.916719 | 42.335694 | -71.075337 | . | std | 3.745629e+04 | 17.188931 | 0.032066 | 0.032259 | . | min | 1.010041e+11 | 2108.000000 | 42.231500 | -71.185400 | . | 25% | 1.010042e+11 | 2119.000000 | 42.314500 | -71.087600 | . | 50% | 1.010042e+11 | 2126.000000 | 42.345900 | -71.062200 | . | 75% | 1.010042e+11 | 2130.000000 | 42.359400 | -71.058700 | . | max | 1.010042e+11 | 2467.000000 | 42.395200 | -70.994900 | . As mentioned before, the case_enquiry_id, location_zipcode, latitude, and longitude columns are not numeric variables. The descriptive statistics are not very useful in this situation. What would be a useful numeric variable is the duration of a request. Let‚Äôs calculate the duration of each of the requests from January 2022 to March 2022 and add it as a new column in our dataframe. # calculating case duration and adding a new column (case_duration) to the dataframe duration = df_filtered['closed_dt'] - df_filtered['open_dt'] df_filtered = df_filtered.assign(case_duration=duration) df_filtered.head() . | | case_enquiry_id | open_dt | target_dt | ... | longitude | source | case_duration | . | 0 | 101004116078 | 2022-01-04 15:34:00 | NaT | ... | -71.0322 | Citizens Connect App | NaT | . | 1 | 101004113538 | 2022-01-01 13:40:13 | 2022-01-04 08:30:00 | ... | -71.0774 | City Worker App | 0 days 03:42:02 | . | 2 | 101004120888 | 2022-01-09 12:40:43 | 2022-01-11 08:30:00 | ... | -71.0683 | City Worker App | 0 days 12:44:07 | . | 3 | 101004120982 | 2022-01-09 13:56:00 | NaT | ... | -71.0256 | Constituent Call | NaT | . | 4 | 101004127209 | 2022-01-15 20:42:00 | 2022-01-20 08:30:00 | ... | -71.0704 | Constituent Call | 0 days 11:36:09 | . 5 rows √ó 29 columns . Now we can see the new case_duration column. Some values are NaT, which means there is a missing date. This makes sense because the case_status is OPEN. Let‚Äôs filter out the open cases and focus on analyzing the duration of the closed cases. # filter out the open cases df_closed = df_filtered.loc[(df_filtered['case_status'] == \"Closed\")] df_closed.head() . | | case_enquiry_id | open_dt | target_dt | ... | longitude | source | case_duration | . | 1 | 101004113538 | 2022-01-01 13:40:13 | 2022-01-04 08:30:00 | ... | -71.0774 | City Worker App | 0 days 03:42:02 | . | 2 | 101004120888 | 2022-01-09 12:40:43 | 2022-01-11 08:30:00 | ... | -71.0683 | City Worker App | 0 days 12:44:07 | . | 4 | 101004127209 | 2022-01-15 20:42:00 | 2022-01-20 08:30:00 | ... | -71.0704 | Constituent Call | 0 days 11:36:09 | . | 5 | 101004113302 | 2022-01-01 00:36:24 | 2022-01-04 08:30:00 | ... | -71.0587 | Citizens Connect App | 1 days 23:36:53 | . | 6 | 101004113331 | 2022-01-01 03:11:23 | NaT | ... | -71.0587 | Constituent Call | 3 days 05:12:07 | . 5 rows √ó 29 columns . With the closed cases, let‚Äôs calculate the descriptive statistics of the new case_duration column. # let's calculate the descriptive statistics again # using double brackets to display in a *fancy* table format df_closed[['case_duration']].describe() . | | case_duration | . | count | 59420 | . | mean | 4 days 12:09:14.466526422 | . | std | 15 days 09:54:44.441079417 | . | min | 0 days 00:00:04 | . | 25% | 0 days 01:26:54.750000 | . | 50% | 0 days 09:01:45 | . | 75% | 1 days 15:40:08.250000 | . | max | 181 days 14:24:23 | . From the table, we can see that the average case duration is ~4.5 days. The standard deviation for the case duration is ~15.4 days. The minimum time a case takes to close is 4 minutes. The maximum time a case takes to close is ~181.6 days. The inter-quartile range (IQR) is the difference between the 25% and 75% quantiles. We can also calculate the mode and median. df_closed['case_duration'].mode() . 0 0 days 00:00:54 1 0 days 00:00:57 2 0 days 00:01:03 Name: case_duration, dtype: timedelta64[ns] . df_closed['case_duration'].median() . Timedelta('0 days 09:01:45') . The descriptive statistics summary in table form is nice, but it would be nice to visualize the data in a histogram. Simply trying to plot using the values in the case_duration column will case an error. Currently, the values in case_duration are of type timedelta64[ns], df_closed['case_duration'] is a Timedelta Series. We will need to apply what is called a frequency conversion to the values. ‚ÄúTimedelta Series, TimedeltaIndex, and Timedelta scalars can be converted to other ‚Äòfrequences‚Äô by dividing by another timedelta, or by astyping to a specific timedelta type.‚Äù (See the link below for more information and code examples!) . https://pandas.pydata.org/pandas-docs/stable/user_guide/timedeltas.html . # dividing the case_duration values by Timedelta of 1 day duration_days = ( df_closed['case_duration'] / pd.Timedelta(days=1)) # adding calculation to dataframe under duration_in_days column df_closed = df_closed.assign(duration_in_days=duration_days) # display descriptive statistics summary with new column addition df_closed[['duration_in_days']].describe() . | | duration_in_days | . | count | 59420.000000 | . | mean | 4.506417 | . | std | 15.413014 | . | min | 0.000046 | . | 25% | 0.060356 | . | 50% | 0.376215 | . | 75% | 1.652873 | . | max | 181.600266 | . # using seaborn library for visualizations sns.set_theme() # use this if you dont want the visualizations to be default matplotlibstyle sns.displot(df_closed, x=\"duration_in_days\", binwidth=1) . &lt;seaborn.axisgrid.FacetGrid at 0x16ae584c0&gt; . From the plot above, the data seems to be skewed right meaning the right tail is much longer than the left. Let‚Äôs try playing with different bin widths. # trying different bin sizes sns.displot(df_closed, x=\"duration_in_days\", binwidth=5) . &lt;seaborn.axisgrid.FacetGrid at 0x16b031790&gt; . # trying different bin sizes sns.displot(df_closed, x=\"duration_in_days\", binwidth=25) . &lt;seaborn.axisgrid.FacetGrid at 0x16b06b340&gt; . Since the data is heavily skewed, let‚Äôs apply log transformation to the data. The log transformation will hopefully reduce or remove the skewness of the original data. The assumption is that the original data follows a log-normal distribution. # log-scale transformation since the data is heavliy skewed # add bin_width parameter to change bin sizes sns.displot(df_closed, x=\"duration_in_days\", log_scale=True) . &lt;seaborn.axisgrid.FacetGrid at 0x16b5577f0&gt; . Which neighborhoods had the most requests from January 2022 - March 2022? . To answer this question, we will take a look at the neighborhood column. # has 25 unique values so a pie chart probably is not the best option len(df_closed['neighborhood'].unique()) . 25 . # plot neighborhood counts sns.countplot(x=\"neighborhood\", data=df_closed).set_title('Number of Requests by Neighborhood') . Text(0.5, 1.0, 'Number of Requests by Neighborhood') . Yikes! The x-axis labels are pretty hard to read. Let‚Äôs fix that by plotting the bars horizontally. # fixing orientation of the labels sns.countplot(y=\"neighborhood\", data=df_closed).set_title('Number of Requests by Neighborhood') . Text(0.5, 1.0, 'Number of Requests by Neighborhood') . From the plot we can see that Dorchester has the most requests, followed by South Boston/South Boston Waterfront, then Roxbury. There‚Äôs a bar that doesn‚Äôt have a name‚Ä¶that‚Äôs strange. Let‚Äôs display the exact counts for each neighborhood. # displaying number of requests by neighborhood in table form df_closed['neighborhood'].value_counts() . Dorchester 9148 South Boston / South Boston Waterfront 5608 Roxbury 5097 East Boston 4420 Allston / Brighton 3945 Jamaica Plain 3696 South End 3666 Downtown / Financial District 3419 Back Bay 2740 Greater Mattapan 2429 Hyde Park 2308 Charlestown 2096 Roslindale 2083 Boston 1803 West Roxbury 1698 Beacon Hill 1595 Fenway / Kenmore / Audubon Circle / Longwood 1034 Mission Hill 990 South Boston 600 476 Brighton 293 Allston 141 Mattapan 72 Chestnut Hill 4 Name: neighborhood, dtype: int64 . There are 476 requests without a neighborhood label. # uncomment and run the line below to check for the empty neighborhood label # print(df_closed['neighborhood'].unique()) # gather the rows where neighborhood == ' ' df_no_neighborhood = df_closed.loc[(df_closed['neighborhood'] == ' ')] df_no_neighborhood.head(15) # display first 15 rows . | | case_enquiry_id | open_dt | target_dt | ... | source | case_duration | duration_in_days | . | 163 | 101004115729 | 2022-01-04 11:11:00 | 2022-02-03 11:11:34 | ... | Constituent Call | 0 days 22:32:58 | 0.939560 | . | 207 | 101004117130 | 2022-01-05 14:25:00 | 2022-01-14 14:25:51 | ... | Constituent Call | 35 days 20:34:15 | 35.857118 | . | 301 | 101004118921 | 2022-01-07 12:55:08 | NaT | ... | Constituent Call | 2 days 20:25:31 | 2.851053 | . | 640 | 101004123032 | 2022-01-11 14:00:00 | 2022-01-25 14:00:53 | ... | Constituent Call | 0 days 22:37:49 | 0.942928 | . | 882 | 101004121696 | 2022-01-10 10:35:00 | 2022-01-17 10:35:33 | ... | Employee Generated | 0 days 00:31:04 | 0.021574 | . | 1280 | 101004141822 | 2022-01-20 12:49:51 | 2022-01-31 12:49:51 | ... | Self Service | 0 days 21:40:40 | 0.903241 | . | 1509 | 101004129011 | 2022-01-18 09:11:00 | 2022-02-01 09:11:09 | ... | Constituent Call | 0 days 01:11:18 | 0.049514 | . | 1574 | 101004144874 | 2022-01-24 09:32:51 | 2022-02-07 09:32:51 | ... | Constituent Call | 0 days 00:22:13 | 0.015428 | . | 1612 | 101004146190 | 2022-01-25 12:48:25 | 2022-02-08 12:48:25 | ... | Constituent Call | 0 days 03:36:25 | 0.150289 | . | 1777 | 101004145555 | 2022-01-24 18:24:00 | 2022-02-03 08:30:00 | ... | Constituent Call | 15 days 15:08:31 | 15.630914 | . | 2433 | 101004115813 | 2022-01-04 12:06:54 | 2022-01-06 12:07:26 | ... | Constituent Call | 0 days 02:50:24 | 0.118333 | . | 2489 | 101004116451 | 2022-01-05 02:15:02 | 2022-01-12 08:30:00 | ... | Constituent Call | 0 days 05:25:57 | 0.226354 | . | 2521 | 101004155729 | 2022-01-31 12:20:00 | 2022-02-07 12:21:11 | ... | Employee Generated | 0 days 02:28:06 | 0.102847 | . | 2677 | 101004156474 | 2022-01-31 15:32:00 | 2022-02-14 15:32:53 | ... | Constituent Call | 3 days 02:58:57 | 3.124271 | . | 2687 | 101004156811 | 2022-01-31 16:48:07 | 2022-02-14 16:48:07 | ... | Constituent Call | 0 days 00:03:00 | 0.002083 | . 15 rows √ó 30 columns . print(df_no_neighborhood['latitude'].unique()) print(df_no_neighborhood['longitude'].unique()) . [42.3594] [-71.0587] . The latitude and longitude values are the same for all of the rows without a neighborhood value. We can use the Geopy module to convert the latitude and longitude coordinates to a place or location address - also referred to as reverse geocoding. # import geopy from geopy.geocoders import Nominatim # make a Nominatim object and initialize, specify a user_agent # Nominatim requires this value to be set to your application name, to be able to limit the number of requests per application # Nominatim is a free service but provides low request limits: https://operations.osmfoundation.org/policies/nominatim/ geolocator = Nominatim(user_agent=\"eda_geotest\") # set latitude and longitude and convert to string lat = str(df_no_neighborhood['latitude'].unique()[0]) long = str(df_no_neighborhood['longitude'].unique()[0]) # get the location information location = geolocator.reverse(lat + \",\" +long) # display location information, add .raw for more details print(location.raw) . {'place_id': 264213803, 'licence': 'Data ¬© OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright', 'osm_type': 'way', 'osm_id': 816277585, 'lat': '42.3594696', 'lon': '-71.05880376899256', 'display_name': \"Sears' Crescent and Sears' Block Building, Franklin Avenue, Downtown Crossing, Downtown Boston, Boston, Suffolk County, Massachusetts, 02201, United States\", 'address': {'building': \"Sears' Crescent and Sears' Block Building\", 'road': 'Franklin Avenue', 'neighbourhood': 'Downtown Crossing', 'suburb': 'Downtown Boston', 'city': 'Boston', 'county': 'Suffolk County', 'state': 'Massachusetts', 'ISO3166-2-lvl4': 'US-MA', 'postcode': '02201', 'country': 'United States', 'country_code': 'us'}, 'boundingbox': ['42.3593149', '42.3596061', '-71.0592779', '-71.0584887']} . Quick Google Maps search of the location confirms that (42.3594, -71.0587) is Government Center. The output from geopy is Sear‚Äôs Crescent and Sears‚Äô Block which are a pair of buildings adjacent to City Hall and City Hall Plaza, Government Center. Another quick look at the output from geopy shows that the lat and lon values are similar but different from the latitude and longitude values in the dataset. The requests without a neighborhood value have a general location of Government Center. At least we can confirm that requests without a neighborhood value are not outside of Boston or erroneous. During January 2022 - March 2022, where did the most case requests come from? . To answer this question, we will take a look at the source column. # has only 5 unique values so in this case we can use a pie chart len(df_closed['source'].unique()) . 5 . # displaying the number of requests by each source type df_closed['source'].value_counts() . Citizens Connect App 32066 Constituent Call 21051 City Worker App 3795 Self Service 1632 Employee Generated 876 Name: source, dtype: int64 . # visualizing the breakdown of where case requests come from # seaborn doesn't have a default pie chart but you can add seaborn color palettes to matplotlib plots colors = sns.color_palette('pastel')[0:5] ax = df_closed['source'].value_counts().plot.pie(colors=colors) . # label each slice with the percentage of requests per source ax = df_closed['source'].value_counts().plot.pie(colors=colors,autopct='%1.1f%%') # run the following to remove the default column name label *source* #ax.yaxis.set_visible(False) . From the pie chart, 54% of the requests from January 2022 - March 2022 came from the Citizens Connect App, 35.4% came from a Constituent Call, followed by 6.4% from the City Worker App. How many different types of requests were there from January 2022 - March 2022? . To answer this question, we will take a look at the reason column. # how many different reasons are there len(df_closed['reason'].unique()) . 38 . # number of requests by reason df_closed['reason'].value_counts() . Enforcement &amp; Abandoned Vehicles 14908 Code Enforcement 10437 Street Cleaning 8477 Sanitation 5993 Highway Maintenance 5032 Signs &amp; Signals 2202 Street Lights 1774 Recycling 1690 Housing 1529 Needle Program 1298 Building 1293 Park Maintenance &amp; Safety 1001 Trees 762 Animal Issues 580 Environmental Services 560 Employee &amp; General Comments 366 Health 344 Graffiti 297 Administrative &amp; General Requests 261 Notification 141 Traffic Management &amp; Engineering 113 Abandoned Bicycle 108 Sidewalk Cover / Manhole 53 Catchbasin 40 Fire Hydrant 26 Noise Disturbance 24 Programs 23 Pothole 22 Air Pollution Control 13 Operations 11 Neighborhood Services Issues 9 Weights and Measures 8 Cemetery 7 Generic Noise Disturbance 7 Parking Complaints 5 Fire Department 3 Office of The Parking Clerk 2 Billing 1 Name: reason, dtype: int64 . There were 38 different types of requests from January 2022 - March 2022, the top three with most requests being Enforcement &amp; Abandoned Vehicles with 14,908 requests, Code Enforcement with 10,437 requests, then Street Cleaning with 8,477 requests. # top case request reason by neighborhood df_closed.groupby(['neighborhood'])['reason'].describe() . | | count | unique | top | freq | . | neighborhood | | | | | . | | 476 | 17 | Employee &amp; General Comments | 278 | . | Allston | 141 | 17 | Code Enforcement | 29 | . | Allston / Brighton | 3945 | 31 | Enforcement &amp; Abandoned Vehicles | 1065 | . | Back Bay | 2740 | 29 | Enforcement &amp; Abandoned Vehicles | 678 | . | Beacon Hill | 1595 | 23 | Street Cleaning | 467 | . | Boston | 1803 | 30 | Enforcement &amp; Abandoned Vehicles | 378 | . | Brighton | 293 | 21 | Enforcement &amp; Abandoned Vehicles | 64 | . | Charlestown | 2096 | 27 | Enforcement &amp; Abandoned Vehicles | 766 | . | Chestnut Hill | 4 | 3 | Health | 2 | . | Dorchester | 9148 | 32 | Enforcement &amp; Abandoned Vehicles | 2195 | . | Downtown / Financial District | 3419 | 28 | Enforcement &amp; Abandoned Vehicles | 807 | . | East Boston | 4420 | 27 | Enforcement &amp; Abandoned Vehicles | 1935 | . | Fenway / Kenmore / Audubon Circle / Longwood | 1034 | 28 | Enforcement &amp; Abandoned Vehicles | 202 | . | Greater Mattapan | 2429 | 27 | Sanitation | 546 | . | Hyde Park | 2308 | 28 | Sanitation | 431 | . | Jamaica Plain | 3696 | 28 | Code Enforcement | 819 | . | Mattapan | 72 | 16 | Street Cleaning | 19 | . | Mission Hill | 990 | 25 | Enforcement &amp; Abandoned Vehicles | 208 | . | Roslindale | 2083 | 30 | Enforcement &amp; Abandoned Vehicles | 387 | . | Roxbury | 5097 | 30 | Enforcement &amp; Abandoned Vehicles | 1019 | . | South Boston | 600 | 19 | Enforcement &amp; Abandoned Vehicles | 243 | . | South Boston / South Boston Waterfront | 5608 | 27 | Enforcement &amp; Abandoned Vehicles | 2530 | . | South End | 3666 | 27 | Code Enforcement | 775 | . | West Roxbury | 1698 | 24 | Sanitation | 450 | . # get counts for each request reason by neighborhood reason_by_neighborhood = df_closed.groupby(['neighborhood', 'reason'])['duration_in_days'].describe()[['count']] reason_by_neighborhood . | | | count | . | neighborhood | reason | | . | | Cemetery | 7.0 | . | Code Enforcement | 9.0 | . | Employee &amp; General Comments | 278.0 | . | Enforcement &amp; Abandoned Vehicles | 17.0 | . | Environmental Services | 3.0 | . | ... | ... | ... | . | West Roxbury | Signs &amp; Signals | 95.0 | . | Street Cleaning | 159.0 | . | Street Lights | 50.0 | . | Traffic Management &amp; Engineering | 4.0 | . | Trees | 72.0 | . 594 rows √ó 1 columns . # run this cell to write the reason by neighborhood to a csv to see all rows of data reason_by_neighborhood.to_csv('reasons_by_neighborhood.csv') . # let's take a look at the South End neighborhood specifically south_end_df = df_closed.loc[(df_closed['neighborhood'] == 'South End')] south_end_df.groupby(['reason'])['duration_in_days'].describe()[['count']] . | | count | . | reason | | . | Abandoned Bicycle | 8.0 | . | Administrative &amp; General Requests | 10.0 | . | Air Pollution Control | 4.0 | . | Animal Issues | 16.0 | . | Building | 52.0 | . | Code Enforcement | 775.0 | . | Employee &amp; General Comments | 1.0 | . | Enforcement &amp; Abandoned Vehicles | 712.0 | . | Environmental Services | 48.0 | . | Fire Hydrant | 2.0 | . | Graffiti | 55.0 | . | Health | 8.0 | . | Highway Maintenance | 360.0 | . | Housing | 40.0 | . | Needle Program | 269.0 | . | Neighborhood Services Issues | 1.0 | . | Noise Disturbance | 2.0 | . | Notification | 2.0 | . | Park Maintenance &amp; Safety | 73.0 | . | Recycling | 29.0 | . | Sanitation | 242.0 | . | Sidewalk Cover / Manhole | 1.0 | . | Signs &amp; Signals | 76.0 | . | Street Cleaning | 717.0 | . | Street Lights | 118.0 | . | Traffic Management &amp; Engineering | 7.0 | . | Trees | 38.0 | . What types of cases typically take the longest to resolve? . To answer this question, let‚Äôs take a look at the duration_in_days and reason columns. # what types of cases typically take the longest # case_duration by reason sns.catplot(x=\"reason\", y=\"duration_in_days\", kind=\"box\", data=df_closed,) . &lt;seaborn.axisgrid.FacetGrid at 0x16ba4bf10&gt; . # The chart is kind of difficult to read... # Let's fix the size of the chart and flip the labels on the x-axis sns.catplot(y=\"reason\", x=\"duration_in_days\", kind=\"box\", data=df_closed, height = 8, aspect = 1.25) . &lt;seaborn.axisgrid.FacetGrid at 0x16bfc04c0&gt; . Box plots display the five-number-summary, which includes: the minimum, the maximum, the sample median, and the first and third quartiles. The box plot shows the distribution duration_in_days in a way that allows comparisions between case reasons. Box plots show the distribution of a numerical variable broken down by a categorical variable. The box shows the quartiles of the duration_in_days and the whiskers extend to show the rest of the distribution (minimum and maximum). Points that are shown outside of the whiskers are determined to be outliers. The line inside the box is the median. # descriptive statistics for duration_in_days by case reason # box plot in table form pd.set_option('display.max_columns', None) df_closed.groupby(['reason'])['duration_in_days'].describe() . | | count | mean | std | min | 25% | 50% | 75% | max | . | reason | | | | | | | | | . | Abandoned Bicycle | 108.0 | 15.100783 | 30.637974 | 0.005799 | 0.610787 | 1.154815 | 5.827731 | 129.944907 | . | Administrative &amp; General Requests | 261.0 | 3.812869 | 13.899890 | 0.000081 | 0.114005 | 0.781192 | 1.984132 | 129.670035 | . | Air Pollution Control | 13.0 | 8.872239 | 10.842552 | 0.001528 | 1.787616 | 1.790822 | 13.153194 | 30.967199 | . | Animal Issues | 580.0 | 0.904125 | 1.086867 | 0.000127 | 0.075793 | 0.705457 | 1.238536 | 11.121447 | . | Billing | 1.0 | 0.009178 | NaN | 0.009178 | 0.009178 | 0.009178 | 0.009178 | 0.009178 | . | Building | 1293.0 | 8.183306 | 16.727784 | 0.000046 | 0.642755 | 2.649641 | 8.094919 | 167.316273 | . | Catchbasin | 40.0 | 6.295217 | 10.713959 | 0.000174 | 0.946927 | 2.444572 | 7.476412 | 61.852523 | . | Cemetery | 7.0 | 33.686310 | 52.072679 | 0.000127 | 0.113744 | 0.594907 | 48.409045 | 138.163553 | . | Code Enforcement | 10437.0 | 0.646142 | 3.440879 | 0.000069 | 0.054375 | 0.234653 | 0.752095 | 146.231354 | . | Employee &amp; General Comments | 366.0 | 3.045888 | 11.407637 | 0.000243 | 0.050307 | 0.607309 | 1.707879 | 104.725579 | . | Enforcement &amp; Abandoned Vehicles | 14908.0 | 4.326244 | 17.119537 | 0.000058 | 0.057263 | 0.177986 | 0.620069 | 168.863750 | . | Environmental Services | 560.0 | 1.750664 | 3.612940 | 0.000069 | 0.500052 | 0.870752 | 1.994499 | 57.933796 | . | Fire Department | 3.0 | 0.598248 | 0.542678 | 0.001887 | 0.365845 | 0.729803 | 0.896429 | 1.063056 | . | Fire Hydrant | 26.0 | 4.028460 | 2.648591 | 0.000914 | 2.174317 | 3.593819 | 5.515179 | 10.837072 | . | Generic Noise Disturbance | 7.0 | 0.809901 | 1.071081 | 0.000498 | 0.010162 | 0.476505 | 1.120243 | 2.931493 | . | Graffiti | 297.0 | 60.795557 | 44.483313 | 0.000278 | 16.802164 | 58.580428 | 95.905475 | 173.824433 | . | Health | 344.0 | 1.524580 | 3.127143 | 0.000058 | 0.143215 | 0.894028 | 1.253898 | 39.648924 | . | Highway Maintenance | 5032.0 | 4.621423 | 14.838073 | 0.000081 | 0.070668 | 0.775781 | 2.315587 | 176.714560 | . | Housing | 1529.0 | 7.676885 | 14.967539 | 0.000058 | 0.692222 | 2.935729 | 8.104514 | 158.114988 | . | Needle Program | 1298.0 | 0.079918 | 0.243719 | 0.000081 | 0.017248 | 0.029554 | 0.053157 | 7.152766 | . | Neighborhood Services Issues | 9.0 | 31.791151 | 31.721388 | 4.324954 | 12.663229 | 24.884931 | 32.983403 | 109.890347 | . | Noise Disturbance | 24.0 | 0.909435 | 1.072071 | 0.000104 | 0.005830 | 0.614433 | 1.358082 | 3.525995 | . | Notification | 141.0 | 19.565133 | 20.603048 | 0.000544 | 1.802569 | 7.719329 | 38.391088 | 64.423113 | . | Office of The Parking Clerk | 2.0 | 2.440463 | 2.261498 | 0.841343 | 1.640903 | 2.440463 | 3.240023 | 4.039583 | . | Operations | 11.0 | 4.725810 | 5.460231 | 0.000266 | 0.697992 | 2.894306 | 7.657731 | 15.175023 | . | Park Maintenance &amp; Safety | 1001.0 | 10.480097 | 21.813287 | 0.000347 | 0.710787 | 1.846771 | 6.700729 | 144.606782 | . | Parking Complaints | 5.0 | 3.200009 | 4.912111 | 0.387477 | 0.388090 | 0.889826 | 2.483206 | 11.851447 | . | Pothole | 22.0 | 3.910974 | 3.141983 | 0.001771 | 1.924826 | 3.009109 | 5.652931 | 14.102245 | . | Programs | 23.0 | 0.658821 | 1.055246 | 0.000336 | 0.001233 | 0.056863 | 0.813414 | 3.142951 | . | Recycling | 1690.0 | 12.245377 | 9.035924 | 0.000081 | 6.856282 | 12.930035 | 16.928313 | 132.917153 | . | Sanitation | 5993.0 | 2.403129 | 3.185305 | 0.000058 | 0.260671 | 0.979294 | 3.836354 | 98.852222 | . | Sidewalk Cover / Manhole | 53.0 | 4.735583 | 8.858981 | 0.000127 | 0.358148 | 2.873461 | 5.035451 | 58.847674 | . | Signs &amp; Signals | 2202.0 | 6.620529 | 14.839239 | 0.000185 | 0.143718 | 1.167795 | 5.073134 | 140.114028 | . | Street Cleaning | 8477.0 | 1.082891 | 6.127272 | 0.000046 | 0.037975 | 0.100706 | 0.605370 | 109.824236 | . | Street Lights | 1774.0 | 18.585520 | 32.664829 | 0.000104 | 0.148079 | 1.984855 | 20.707622 | 181.600266 | . | Traffic Management &amp; Engineering | 113.0 | 11.388050 | 25.003672 | 0.000729 | 0.085231 | 0.842176 | 6.080509 | 125.919988 | . | Trees | 762.0 | 25.210341 | 41.285189 | 0.000139 | 0.018481 | 0.804022 | 39.169418 | 163.697106 | . | Weights and Measures | 8.0 | 0.985169 | 0.558955 | 0.107234 | 0.741134 | 0.900399 | 1.207185 | 1.808912 | . Graffiti cases take on average take the longests time to resolve, 60.796 days. Do cases typically take longer in one neighborhood over another? . # do cases typically take longer in one neighborhood over another? sns.catplot(y=\"neighborhood\", x=\"duration_in_days\", kind=\"box\", data=df_closed, height = 8, aspect = 1.25) . &lt;seaborn.axisgrid.FacetGrid at 0x16c38f880&gt; . The box plot above shows several outliers for each category (neighborhood) making it difficult to read and quite overwhelming. Let‚Äôs display the information in table form. # in table form df_closed.groupby(['neighborhood'])['duration_in_days'].describe() . | | count | mean | std | min | 25% | 50% | 75% | max | . | neighborhood | | | | | | | | | . | | 476.0 | 4.178610 | 13.672122 | 0.000127 | 0.058209 | 0.705318 | 2.208099 | 138.163553 | . | Allston | 141.0 | 3.947410 | 9.929540 | 0.000058 | 0.134988 | 0.873414 | 2.763426 | 63.832847 | . | Allston / Brighton | 3945.0 | 4.622345 | 14.786827 | 0.000185 | 0.075984 | 0.511424 | 1.995000 | 161.749630 | . | Back Bay | 2740.0 | 4.900469 | 16.733563 | 0.000197 | 0.043553 | 0.215110 | 1.172946 | 166.873588 | . | Beacon Hill | 1595.0 | 2.485273 | 10.776832 | 0.000301 | 0.042506 | 0.167801 | 0.863617 | 136.561204 | . | Boston | 1803.0 | 5.152869 | 17.959729 | 0.000046 | 0.041944 | 0.345833 | 1.555781 | 156.934120 | . | Brighton | 293.0 | 5.162394 | 14.575959 | 0.000266 | 0.078623 | 0.547211 | 2.031343 | 126.706586 | . | Charlestown | 2096.0 | 4.147373 | 14.840357 | 0.000081 | 0.067653 | 0.278171 | 1.162135 | 139.983704 | . | Chestnut Hill | 4.0 | 19.075249 | 30.039288 | 0.148611 | 0.185148 | 6.448744 | 25.338845 | 63.254896 | . | Dorchester | 9148.0 | 3.956899 | 12.579649 | 0.000046 | 0.068087 | 0.515260 | 1.883400 | 156.208553 | . | Downtown / Financial District | 3419.0 | 3.475034 | 13.957725 | 0.000231 | 0.050648 | 0.234988 | 0.973443 | 176.714560 | . | East Boston | 4420.0 | 2.777781 | 10.877246 | 0.000058 | 0.037833 | 0.181094 | 0.902786 | 150.815174 | . | Fenway / Kenmore / Audubon Circle / Longwood | 1034.0 | 7.722932 | 19.772496 | 0.000150 | 0.101777 | 0.636291 | 2.556918 | 165.773264 | . | Greater Mattapan | 2429.0 | 4.807563 | 14.036534 | 0.000104 | 0.084375 | 0.678646 | 2.818472 | 158.114988 | . | Hyde Park | 2308.0 | 6.664146 | 16.259478 | 0.000127 | 0.081476 | 0.747078 | 5.331976 | 157.788715 | . | Jamaica Plain | 3696.0 | 6.216330 | 18.281044 | 0.000104 | 0.081823 | 0.569161 | 2.147717 | 148.726493 | . | Mattapan | 72.0 | 3.742091 | 11.557064 | 0.000069 | 0.071858 | 0.725631 | 2.012494 | 74.130579 | . | Mission Hill | 990.0 | 5.582632 | 18.433571 | 0.000081 | 0.055107 | 0.281186 | 1.690460 | 137.808854 | . | Roslindale | 2083.0 | 7.048947 | 21.030090 | 0.000289 | 0.109369 | 0.714190 | 2.658108 | 173.824433 | . | Roxbury | 5097.0 | 5.020254 | 17.307327 | 0.000069 | 0.070324 | 0.495382 | 1.817338 | 161.948993 | . | South Boston | 600.0 | 3.175501 | 14.327518 | 0.000058 | 0.055269 | 0.228287 | 1.015081 | 159.967488 | . | South Boston / South Boston Waterfront | 5608.0 | 3.160470 | 14.300232 | 0.000150 | 0.051296 | 0.181863 | 0.827731 | 181.600266 | . | South End | 3666.0 | 4.225789 | 16.643360 | 0.000243 | 0.043921 | 0.187778 | 0.967399 | 167.869965 | . | West Roxbury | 1698.0 | 6.004628 | 17.783995 | 0.000081 | 0.106288 | 0.787685 | 3.035735 | 161.054688 | . In January 2022 - March 2022, cases took the longest in Chestnut Hill. Cases typically lasted on average 19.075 days but there were only 4 cases located in Chestnut Hill during this time. Smaller sample sizes could mean more variability (look at standard deviation to explain the spread of observations). We can further look at the population of Chestnut Hill versus the other neighborhoods to try and make sense of this low case count. Additionally, we can broaden the time period of the cases to see if Chestnut Hill still has a low case count. From the table above we can see how long cases take by each neighborhood, it would be interesting to further breakdown by case reason for each neighborhood. Wrap Up, Next Steps . Further analysis could be done using the 311 dataset. Using the 311 data from previous years, we can see how number of requests have changed over the years, or how case duration may have changed over the years. Since most requests have latitude and longitude coordinates it could be interesting to plot each case request on a map to see if there are clusters of requests in certain locations. Next steps could include gathering demographic data to overlay on top of the 311 dataset for further analysis. Another possible next step would be to build a model to predict how long a request could take given the request reason, subject, location, source, etc. Click here to download this Jupyter Notebook (make sure you are signed in with your BU email)! . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/eda_example.html",
    "relUrl": "/documentation/project-guides/eda_example.html"
  },"20": {
    "doc": "Exploratory Data Analysis (EDA)",
    "title": "Exploratory Data Analysis (EDA)",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-guides/eda_final.html",
    "relUrl": "/documentation/project-guides/eda_final.html"
  },"21": {
    "doc": "Exploratory Data Analysis (EDA)",
    "title": "What EDA?",
    "content": "Exploratory data analysis (EDA) is an iterative process of analyzing data sets to summarize their main characteristics through visualizations, graphical and/or numerical summaries. EDA is primarily used for seeing what the data can tell us beyond the formal modeling. The goal of EDA is to explore the data to gain a better understanding of the data before proceeding to formal analysis. EDA can help identify obvious errors, understand any patterns within the data, detect outliers or anomalous points, and find interesting relationships among the variables. The insights drawn from EDA can then be used for more sophisticated data analysis or modeling, including machine learning. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/eda_final.html#what-eda",
    "relUrl": "/documentation/project-guides/eda_final.html#what-eda"
  },"22": {
    "doc": "Exploratory Data Analysis (EDA)",
    "title": "Popular Python Libraries for Data Science",
    "content": ". | NumPy | Pandas | Scikit-learn | Matplotlib | Seaborn | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/eda_final.html#popular-python-libraries-for-data-science",
    "relUrl": "/documentation/project-guides/eda_final.html#popular-python-libraries-for-data-science"
  },"23": {
    "doc": "Exploratory Data Analysis (EDA)",
    "title": "Initial Questions To Consider",
    "content": "How many observations/rows are there? . How many variables/columns are there? . What kinds of variables are there? Categorical? Numerical? Both? . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/eda_final.html#initial-questions-to-consider",
    "relUrl": "/documentation/project-guides/eda_final.html#initial-questions-to-consider"
  },"24": {
    "doc": "Exploratory Data Analysis (EDA)",
    "title": "Variable Types",
    "content": "Qualitative Variables . A qualitative variable is also referred to as a categorical variable and takes on one value of a fixed number of possible values. Examples of qualitative variabels include: . | Level of education (High School Graduate, Associate‚Äôs Degree, Bachelor‚Äôs Degree, Master‚Äôs Degree) | Political Affiliation (Democrat, Republican, Independent) | Neighborhood in Boston | . Quantitative Variables . A quantitative variable is also referred to a numeric variable and represents a measured quantity. Examples of quantitative variables include: . | Population of a city | Number of individuals in a household | Age of an individual | Height of an individual | Inches of rainfall in a day | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/eda_final.html#variable-types",
    "relUrl": "/documentation/project-guides/eda_final.html#variable-types"
  },"25": {
    "doc": "Exploratory Data Analysis (EDA)",
    "title": "Descriptive Statistics",
    "content": "Descriptive statistics provide simple summaries about the set of observations. They can serve as a sanity check to make sure that your data makes sense or if it doesn‚Äôt make sense identify potential errors in the data. Examples of descriptive statistics are: . | Estimates of central tendency (e.g. mean, median) . | These values describe where most observations in the data set lie. | The mean is commonly used, but is overly influenced by extreme outliers. | The median is a more robust estimator of central tendency that is less influenced by extreme outliers. | The median does not consider the precise value of each observation and may not be where the bulk of the data is. | . | Spread (e.g. standard deviation, inter-quartile range) . | These values describe how far the observations are scattered around the central tendency. | Standard deviation quantifies spread about the mean. Similar to the mean, standard deviation is influenced by outliers. | Inter-quartile range quantifies spread about the median. | Inter-quartile range is the difference between the 25% and 75% quantiles. | Inter-quartile range is a more robust to outliers than standard deviation. | . | Extremes (e.g. min and max values) . | Minimum value of the quantitative variable. | Maximum value of the quantitative variable. | Are these min and max values expected for this variable? Do they make sense? | . | . Questions to consider when looking at descriptive statistics: . | Which values are most common? Why is this the case? | Which values are uncommon? Does this make sense? Does this match your expectations? | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/eda_final.html#descriptive-statistics",
    "relUrl": "/documentation/project-guides/eda_final.html#descriptive-statistics"
  },"26": {
    "doc": "Exploratory Data Analysis (EDA)",
    "title": "Visualizations",
    "content": "Visualizations can uncover more insights about your data and can serve as a great mechanism for explaining your findings to others. Visualizations are useful in examining the distributions of variables, either categorical or numerical. | Bar charts can be used to examine the distribution of a categorical variable, where the x-axis are the different categories the variable can be and the y-axis are the counts. | Bar charts give a visual representation of the frequency of the different groups. | . | Histograms can be used to examine the distribution of a numerical variable. | Explore using a variety of bin widths when working with a histogram. Different bin widths may reveal different patterns. | . | Box plots can be used to display the distribution of a numerical variable broken down by a categorical variable. This is useful for visualizing a combination of variable types. | Scatter plots can be used to examine if a linear relationship exists between two quantitative variables. | Correlation measures the strength of the linear relationship between two quantitative variables. | The correlation coefficient can take a value between -1 and 1. | A correleation coefficient close to -1 is a strong negative relationship. | A correlation coefficient close to 0 is a weak relationship. | A correlation coefficient close to 1 is a strong positive relationship. | . | . | . | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/eda_final.html#visualizations",
    "relUrl": "/documentation/project-guides/eda_final.html#visualizations"
  },"27": {
    "doc": "Getting Started With Geographical Exploratory Data Analysis",
    "title": "Getting Started With GeoPandas",
    "content": "This notebook serves as a starter guide or template for doing geographical analysis using the GeoPandas library. The dataset we will be using in this tutorial is from Analyze Boston. Analyze Boston is the City of Boston‚Äôs data hub and is a great resource for data sets regarding the city. We will be working with two datasets. First is the 2020 CENSUS TRACTS IN BOSTON dataset, and second is the WICKED FREE WIFI LOCATIONS dataset. The first dataset contains the geographical boundaries of each census tract in Boston, and the second dataset contains the geographical location of each free wifi location in Boston. Census Tracts in Boston . Wicked Free Wifi Locations . # Start by importing the necessary packages import pandas as pd import geopandas as gpd from shapely.geometry import Point, Polygon # for visualizations : import seaborn as sns import matplotlib.pyplot as plt . When working with geographical data, .shp files are the most common format, however these files are often accompanied by other files with the same name but different extensions. For example, a .shp file might come with a .dbf, .prj, .shx, and .cpg file. These are necessary files and are used in the background. GeoPandas (and other GIS software) typically expect these files to be in the same directory as the .shp file and to share the same base filename. # read in the census data census = gpd.read_file('census/Census2020_BlockGroups.shp') census.head() . | | OBJECTID | STATEFP20 | COUNTYFP20 | TRACTCE20 | BLKGRPCE20 | GEOID20 | NAMELSAD20 | MTFCC20 | FUNCSTAT20 | ALAND20 | AWATER20 | INTPTLAT20 | INTPTLON20 | Shape_STAr | Shape_STLe | geometry | . | 0 | 1 | 25 | 025 | 040600 | 1 | 250250406001 | Block Group 1 | G5030 | S | 1265377.0 | 413598.0 | +42.3833695 | -071.0707743 | 1.807118e+07 | 29256.866068 | POLYGON ((769378.692 2964626.314, 769385.078 2... | . | 1 | 2 | 25 | 025 | 051101 | 1 | 250250511011 | Block Group 1 | G5030 | S | 220626.0 | 0.0 | +42.3882285 | -071.0046816 | 2.374654e+06 | 9142.174252 | POLYGON ((788317.786 2966115.262, 788838.563 2... | . | 2 | 3 | 25 | 025 | 051101 | 4 | 250250511014 | Block Group 4 | G5030 | S | 227071.0 | 270.0 | +42.3913407 | -071.0020343 | 2.446949e+06 | 11579.546171 | POLYGON ((789538.125 2967889.427, 789539.214 2... | . | 3 | 4 | 25 | 025 | 981600 | 1 | 250259816001 | Block Group 1 | G5030 | S | 586981.0 | 158777.0 | +42.3886205 | -070.9934424 | 8.026752e+06 | 16626.718823 | POLYGON ((790938.417 2966482.118, 790974.619 2... | . | 4 | 5 | 25 | 025 | 010204 | 3 | 250250102043 | Block Group 3 | G5030 | S | 145888.0 | 0.0 | +42.3459611 | -071.1020344 | 1.570220e+06 | 5510.560013 | POLYGON ((762928.668 2951612.031, 763063.607 2... | . What makes a .shp file unique is the ‚Äúgeometry‚Äù column. The geometry column typically contains the coordinates or geometric properties of the features represented in the shapefile, such as points, lines, or polygons. For us, each row in our shapefile represents one census block group, and the geometry column stores a vectorized polygon boundaries of each block group that lets us see a visual representation of the block group. census['geometry'][0] . We can plot all of these block groups together to get a visual representation of the city of Boston. census.plot() plt.title('Boston Census Block Groups') plt.show() . Now let‚Äôs read in our Wicked Free Wifi Locations shapefile . # read in the wifi locations shapefile wfw = gpd.read_file('wifi/Wicked_Free_WiFi_Locations.shp') wfw . | | neighborho | neighbor_1 | device_ser | device_con | device_add | device_lat | device_lon | device_tag | etl_update | is_current | org1 | org2 | inside_out | landmark | ObjectId | geometry | . | 0 | N_568579452955534204 | Dorchester | Q2CK-H48M-P2E9 | DOT-BFD21-AP3 | 641 Columbia Rd., Dorchester, MA | 42.318336 | -71.063236 | BFD Engine-21 Outside | 2023-06-12 | 1 | BFD | NaN | Outside | Engine 21 | 1 | POINT (-7910723.209 5208784.596) | . | 1 | N_568579452955537848 | South Boston | Q2CK-LEG8-FAFN | SB-HIGHSCHOOL-AP2 | 95 G St., South Boston, MA | 42.332869 | -71.044891 | recently-added | 2023-06-12 | 1 | NaN | NaN | NaN | NaN | 2 | POINT (-7908681.044 5210972.710) | . | 2 | N_568579452955538316 | Hyde Park | Q2CK-NERF-4JX7 | HP-BPDE18-AP2 | 1249 Hyde Park Ave., Hyde Park, MA | 42.256473 | -71.124219 | recently-added | 2023-06-12 | 1 | NaN | NaN | NaN | NaN | 3 | POINT (-7917511.803 5199475.676) | . | 3 | N_579275502070532581 | Roxbury | Q2CK-DSFT-7HTL | ROX-TROTTER-AP3 | Humbolt &amp; Waumbeck St., Roxbury, MA | 42.313249 | -71.089180 | BPS Outside Trotter-School | 2023-06-12 | 1 | BPS | NaN | Outside | Trotter School | 4 | POINT (-7913611.287 5208018.690) | . | 4 | N_579275502070532581 | Roxbury | Q2CK-2D9P-VKGR | ROX-BFDE42-AP1 | 1870 Columbus Ave., Roxbury, MA | 42.318412 | -71.097758 | BFD Engine-42 Outside | 2023-06-12 | 1 | BFD | NaN | Outside | Engine 42 | 5 | POINT (-7914566.172 5208795.947) | . | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . | 255 | N_568579452955534204 | Dorchester | Q2CK-G9M2-VTK4 | DOT-BFD21-AP2 | 641 Columbia Rd., Dorchester, MA | 42.318336 | -71.063236 | BFD Engine-21 Outside | 2023-06-12 | 1 | BFD | NaN | Outside | Engine 21 | 256 | POINT (-7910723.209 5208784.596) | . | 256 | N_568579452955527921 | Parks | Q2CK-MP93-YDN3 | PARKS-COMMONEAST-AP1 | 139 Tremont St, Boston, MA 02111 | 42.355436 | -71.063827 | Boston-Common | 2023-06-12 | 1 | NaN | NaN | NaN | Boston Common | 257 | POINT (-7910789.066 5214371.556) | . | 257 | L_601230550253962688 | Navy Yard | Q2EK-HAXQ-8XW4 | NavyYard-AP2 | Navy yard Cambridge | 42.373720 | -71.053272 | NaN | 2023-06-12 | 1 | NaN | NaN | NaN | NaN | 258 | POINT (-7909614.112 5217126.360) | . | 258 | N_568579452955534204 | Dorchester | Q2CK-NJQU-RB55 | DOT-MATHER-AP3 | 24 Parrish St, Dorchester | 42.308397 | -71.061017 | recently-added | 2023-06-12 | 1 | NaN | NaN | NaN | NaN | 259 | POINT (-7910476.256 5207288.280) | . | 259 | N_579275502070532581 | Roxbury | Q2CK-NEZE-VRYJ | ROX-VINE-AP3 | 339 Dudley St, Roxbury | 42.326811 | -71.076707 | recently-added | 2023-06-12 | 1 | NaN | NaN | NaN | NaN | 260 | POINT (-7912222.881 5210060.516) | . 260 rows √ó 16 columns . This time, our geometry column are single POINTs rather than whole POLYGONs, which makes sense because each row represents a single wifi location. After plotting them all together we see just a bunch of dots, which is not very helpful. We can make this plot more useful by adding the census block groups to the plot as well. wfw.plot() plt.title('Wifi Locations') plt.show() . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/geo_eda.html#getting-started-with-geopandas",
    "relUrl": "/documentation/project-guides/geo_eda.html#getting-started-with-geopandas"
  },"28": {
    "doc": "Getting Started With Geographical Exploratory Data Analysis",
    "title": "Data Cleaning",
    "content": "print(census.shape) print(wfw.shape) . (581, 16) (260, 16) . census.info() . &lt;class 'geopandas.geodataframe.GeoDataFrame'&gt; RangeIndex: 581 entries, 0 to 580 Data columns (total 16 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 OBJECTID 581 non-null int64 1 STATEFP20 581 non-null object 2 COUNTYFP20 581 non-null object 3 TRACTCE20 581 non-null object 4 BLKGRPCE20 581 non-null object 5 GEOID20 581 non-null object 6 NAMELSAD20 581 non-null object 7 MTFCC20 581 non-null object 8 FUNCSTAT20 581 non-null object 9 ALAND20 581 non-null float64 10 AWATER20 581 non-null float64 11 INTPTLAT20 581 non-null object 12 INTPTLON20 581 non-null object 13 Shape_STAr 581 non-null float64 14 Shape_STLe 581 non-null float64 15 geometry 581 non-null geometry dtypes: float64(4), geometry(1), int64(1), object(10) memory usage: 72.8+ KB . wfw.info() . &lt;class 'geopandas.geodataframe.GeoDataFrame'&gt; RangeIndex: 260 entries, 0 to 259 Data columns (total 16 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 neighborho 260 non-null object 1 neighbor_1 260 non-null object 2 device_ser 260 non-null object 3 device_con 250 non-null object 4 device_add 245 non-null object 5 device_lat 248 non-null float64 6 device_lon 248 non-null float64 7 device_tag 215 non-null object 8 etl_update 260 non-null object 9 is_current 260 non-null int64 10 org1 119 non-null object 11 org2 5 non-null object 12 inside_out 162 non-null object 13 landmark 168 non-null object 14 ObjectId 260 non-null int64 15 geometry 248 non-null geometry dtypes: float64(2), geometry(1), int64(2), object(11) memory usage: 32.6+ KB . Doesn‚Äôt look like there‚Äôs much to clean in the census data, but in the wifi dataset let‚Äôs just drop the organization columns because they contain the most nulls, and we don‚Äôt need them for our analysis. wfw.drop(['org1', 'org2'], axis=1, inplace=True) . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/geo_eda.html#data-cleaning",
    "relUrl": "/documentation/project-guides/geo_eda.html#data-cleaning"
  },"29": {
    "doc": "Getting Started With Geographical Exploratory Data Analysis",
    "title": "Showing the wifi locations on our census block group map",
    "content": "So we can see a map of Boston and a map of dots representing wifi locations separately, but how do I overlay these dots on top of the map of Boston to actuall see where in Boston these wifi locations are? First, check that the Coordinate Reference Systems (CRS) of both geodataframes are the same. The CRS specifies the coordinate system and reference frame used to define the spatial positions of the features represented in the file, and in order to overlay two geodataframes, they must have the same CRS. # Check the CRS of both GeoDataFrames print(\"Census CRS: \", census.crs) print(\"Wifi CRS: \", wfw.crs) . Census CRS: PROJCS[\"NAD83 / Massachusetts Mainland (ftUS)\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"latitude_of_origin\",41],PARAMETER[\"central_meridian\",-71.5],PARAMETER[\"standard_parallel_1\",41.7166666666667],PARAMETER[\"standard_parallel_2\",42.6833333333333],PARAMETER[\"false_easting\",656166.666666667],PARAMETER[\"false_northing\",2460625],UNIT[\"US survey foot\",0.304800609601219,AUTHORITY[\"EPSG\",\"9003\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]] Wifi CRS: EPSG:3857 . Since they‚Äôre different, we need to use the to_crs method to make sure they‚Äôre all in the same coordinate system. We‚Äôll use the EPSG:4326 coordinate system, which is one of the most common coordinate systems for geographical data. Then, we can overlay the two plots using matplotlib. # If they're different, reproject the wifi data to match the census data if census.crs != wfw.crs: census = census.to_crs(wfw.crs) # Create a new figure and axis fig, ax = plt.subplots(1, 1) # Plot the census data census.plot(ax=ax, color='gray') # Plot the wifi locations wfw.plot(ax=ax, color='red', markersize=10, alpha=0.2) plt.title(\"Boston Census Block Groups and Wifi Locations\") plt.show() . If you want to zoom in on a specific area of a map in GeoPandas, you need to add the lines: plt.xlim() and plt.ylim(). These functions take in the xmin and xmax, and ymin and ymax values of the area you want to zoom in on. You can find these values by looking at the x and y axes of the original plot. # Same as before if census.crs != wfw.crs: census = census.to_crs(wfw.crs) fig, ax = plt.subplots(1, 1) census.plot(ax=ax, color='gray') wfw.plot(ax=ax, color='red', markersize=10, alpha=0.2) plt.title(\"Zoomed in Boston Census Block Groups and Wifi Locations\") # Set the x and y limits to zoom in on Boston plt.xlim([-7.915e6, -7.91e6]) # xmin and xmax plt.ylim([5.2075e6, 5.2125e6]) # ymin and ymax plt.show() . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/geo_eda.html#showing-the-wifi-locations-on-our-census-block-group-map",
    "relUrl": "/documentation/project-guides/geo_eda.html#showing-the-wifi-locations-on-our-census-block-group-map"
  },"30": {
    "doc": "Getting Started With Geographical Exploratory Data Analysis",
    "title": "Make a Choropleth (color each census block according to the number of WiFi locations in it):",
    "content": "Now that both of our GeoDataFrames are on the same CRS, we can do a spatial join to find out how many WiFi locations are in each census tract. A spatial join is a type of join that merges two GeoDataFrames based on the spatial relationship between the geometries of the two GeoDataFrames. In this case, we want to know how many WiFi locations are in each census tract, so we will use the contains operation to find all of the WiFi locations that are within each census tract. # Perform a spatial join between the census tracts and WiFi locations wifi_per_tract = gpd.sjoin(census, wfw, predicate='contains').groupby('TRACTCE20').size() wifi_per_tract.head() . TRACTCE20 000301 1 000603 3 000604 1 000805 1 030302 35 dtype: int64 . The ‚Äúcontains‚Äù operation (as well as other spatial operations like ‚Äúintersects‚Äù, ‚Äúwithin‚Äù, etc.) uses geometric calculations based on the shapes‚Äô coordinates (which have the same CRS) to determine spatial relationships. For the ‚Äúcontains‚Äù operation specifically, it checks if all points of the other geometric object are within the current geometric object, and that their boundaries do not coincide. For example: . from shapely.geometry import Point, Polygon # Create a 5x5 square (tract) polygon = Polygon([(0, 0), (5, 0), (5, 5), (0, 5)]) # Create two points (WiFi location) point1 = Point(3, 3) # Inside the polygon point2 = Point(6, 6) # Outside the polygon # Check containment print(polygon.contains(point1)) print(polygon.contains(point2)) . True False . We can now groupby the unique ‚ÄòTRACTCE20‚Äô values and take .size() to get the number of wifi locations in each tract. Then, we can merge this with our census dataframe to get a new column with the number of wifi locations in each census tract. # Add a new column to the census GeoDataFrame with the count of WiFi locations per tract census = census.merge(wifi_per_tract.rename('wifi_count'), how='left', left_on='TRACTCE20', right_index=True) # Replace NaN values (tracts without any WiFi location) with 0 census['wifi_count'].fillna(0, inplace=True) # Now, create a new figure and axis fig, ax = plt.subplots(1, 1, figsize=(10, 10)) # Plot the census data, color-coded by the number of WiFi locations census.plot(column='wifi_count', ax=ax, legend=True, cmap='cividis', legend_kwds={'label': \"Number of WiFi Locations per Tract\", 'orientation': \"horizontal\"}) plt.title(\"Number of WiFi Locations per Tract\") plt.show() . census[census['wifi_count'] == census['wifi_count'].max()]['wifi_count'] . 478 35.0 Name: wifi_count, dtype: float64 . Looks like there‚Äôs 35 free wifi locations in this bright yellow tract, maybe the ‚Äòlandmarks‚Äô column will tell us why‚Ä¶ . wfw['landmark'].value_counts().head(5).sort_values().plot(kind='barh', figsize=(10, 10)) plt.title('Number of WiFi Locations by Landmark') plt.xlabel('Number of WiFi Locations') plt.ylabel('Landmark') plt.show() . So it could be the City Hall, let‚Äôs graph it! . city_hall_wifi = wfw[wfw['landmark'] == 'City Hall'] if census.crs != city_hall_wifi.crs: city_hall_wifi = city_hall_wifi.to_crs(census.crs) fig, ax = plt.subplots() census.plot(ax=ax, color='grey', edgecolor='black') city_hall_wifi.plot(ax=ax, color='red', markersize=10) plt.title('WiFi Locations at City Hall') plt.show() . Looks like we‚Äôre right! After plotting the subset of our dataframe where Landmark == ‚ÄúCity Hall‚Äù, we can see that all of these points are clustered in that bright yellow census tract. Looking at the listed address of these WiFi locations, we can see that they are all located at 1 City Hall Square, which is the address of Boston City Hall. Here are the unique device addresses of these city hall wifi locations: . city_hall_wifi['device_add'].unique() . array(['1 City Hall Square, Boston, MA 02201', '1 City Hall Plaza, Pavilion\\nBoston, MA 02201', 'One City Hall Sq, Pavilion Bldg\\nBoston, MA 02201', nan, '1 City Hall Plaza, \\nBoston, MA 02201', 'One City Hall Sq, Pavilion Bld, IWF Room,\\nBoston, MA 02201'], dtype=object) . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/geo_eda.html#make-a-choropleth-color-each-census-block-according-to-the-number-of-wifi-locations-in-it",
    "relUrl": "/documentation/project-guides/geo_eda.html#make-a-choropleth-color-each-census-block-according-to-the-number-of-wifi-locations-in-it"
  },"31": {
    "doc": "Getting Started With Geographical Exploratory Data Analysis",
    "title": "Inside vs Outside Wifi locations",
    "content": "fig, ax = plt.subplots(1, 1) census.plot(ax=ax, color='gray') # Plot the wifi locations with color based on \"inside_out\" column wfw.plot(ax=ax, column='inside_out', cmap='cool', markersize=10, legend=True, alpha=0.2) plt.title(\"Boston Census Block Groups and Wifi Locations\") plt.show() . wfw['inside_out'].value_counts().plot(kind='barh') plt.show() . Looks like most of our WiFi locations are outside. There‚Äôs probably a lot of ‚Äúregular‚Äù EDA that can be done here, but since this is a GeoPandas/geographical analysis notebook, we‚Äôll just leave it at that. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/geo_eda.html#inside-vs-outside-wifi-locations",
    "relUrl": "/documentation/project-guides/geo_eda.html#inside-vs-outside-wifi-locations"
  },"32": {
    "doc": "Getting Started With Geographical Exploratory Data Analysis",
    "title": "Closest Wifi location to BU",
    "content": "from geopy.distance import geodesic # Coordinates of Boston University (accordign to Google) BU_latitude = 42.3505 BU_longitude = -71.1054 # Create a copy of the WiFi locations and drop rows with missing coordinates wfw = wfw.copy() wfw.dropna(subset=['device_lat', 'device_lon'], inplace=True) # Function to calculate distances to BU def calculate_distance(row): point = (row['device_lat'], row['device_lon']) return geodesic(point, (BU_latitude, BU_longitude)).miles wfw['distance_to_BU'] = wfw.apply(calculate_distance, axis=1) # Find the WiFi location with the minimum distance wfw[wfw['distance_to_BU'] == min(wfw['distance_to_BU'])] . | | neighborho | neighbor_1 | device_ser | device_con | device_add | device_lat | device_lon | device_tag | etl_update | is_current | inside_out | landmark | ObjectId | geometry | distance_to_BU | . | 36 | N_601230550253961587 | BCYF Tremont | Q2FD-4SE4-JW2S | BCYF-3rd FL GED | 1483 Tremont St, Boston, MA | 42.332317 | -71.09865 | BCYF Inside employee | 2023-06-12 | 1 | Inside | employee | 37 | POINT (-7914665.525 5210889.576) | 1.301774 | . In order to find the closest wifi location to BU, we need to find the distance between each wifi location and BU. We can do this using the geodesic function from geopy, which returns the distance between two longitudes/latitudes. Then, we can find the row where the distance is the smallest, which will be the closest wifi location to BU. The closest wifi location to BU is the one at 1483 Tremont St, Boston, MA. It‚Äôs about 1.3 miles away from BU. Let‚Äôs plot it! . fig, ax = plt.subplots(1, 1) # Plot the census data census.plot(ax=ax, color='gray') # Plot the wifi locations wfw.plot(ax=ax, color='blue', markersize=10, alpha=0.2) # Plot the WiFi location closest to BU in red wfw.query(\"device_ser == 'Q2FD-4SE4-JW2S'\").plot(ax=ax, color='red', markersize=20) plt.title(\"1483 Tremont Street and other Wifi Locations\") plt.show() . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/geo_eda.html#closest-wifi-location-to-bu",
    "relUrl": "/documentation/project-guides/geo_eda.html#closest-wifi-location-to-bu"
  },"33": {
    "doc": "Getting Started With Geographical Exploratory Data Analysis",
    "title": "Wrap Up, Next Steps",
    "content": "There‚Äôs a lot more you can do with these datasets, for example, you could try finding the average distance between wifi locations, or incorporate another dataset with demographic information and try to find a relationship between income, education level, population density, etc and wifi locations. but this notebook should serve as a good starting point for your geographical analysis. Click here to download this Jupyter Notebook and data (make sure you are signed in with your BU email)! . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/geo_eda.html#wrap-up-next-steps",
    "relUrl": "/documentation/project-guides/geo_eda.html#wrap-up-next-steps"
  },"34": {
    "doc": "Getting Started With Geographical Exploratory Data Analysis",
    "title": "Getting Started With Geographical Exploratory Data Analysis",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-guides/geo_eda.html",
    "relUrl": "/documentation/project-guides/geo_eda.html"
  },"35": {
    "doc": "Github Administration",
    "title": "What is this document",
    "content": "This document will guide you through administration of a Spark! owned Github repository, adding and managing collaborators and basic branch use. ",
    "url": "https://bu-spark.github.io/docs/github-admin/#what-is-this-document",
    "relUrl": "/docs/github-admin/#what-is-this-document"
  },"36": {
    "doc": "Github Administration",
    "title": "Introduction",
    "content": "Spark! utilizes Github.com to store and manage all code produced from our various projects. It is treated as the single source of truth for code from projects and is used both during active development and for archival purposes. ",
    "url": "https://bu-spark.github.io/docs/github-admin/#introduction",
    "relUrl": "/docs/github-admin/#introduction"
  },"37": {
    "doc": "Github Administration",
    "title": "Quick Summary",
    "content": "To add users to a repository utilize the COLLABORATORS file located in the root of the repository. Put a single Github username per line and commit to main or master. In the case that either branch is protected you can commit to dev. A Github Action will run automatically and send invites to the new users and remove users who are no longer on the list. ",
    "url": "https://bu-spark.github.io/docs/github-admin/#quick-summary",
    "relUrl": "/docs/github-admin/#quick-summary"
  },"38": {
    "doc": "Github Administration",
    "title": "In depth",
    "content": "This part will go over in more detail the process and intricacies of adding and managing users. ",
    "url": "https://bu-spark.github.io/docs/github-admin/#in-depth",
    "relUrl": "/docs/github-admin/#in-depth"
  },"39": {
    "doc": "Github Administration",
    "title": "Types of Repositories",
    "content": "Repositories can be of two types, private or public. Private repositories can only be viewed by those who have been explicitly added whereas public repositories can be viewed by anyone (including non-BU affiliated individuals). In both cases, users must be added as collaborators in order to contribute code to the repository. Look for the label next to the name of the repository on Github to identify if it is public or private. ",
    "url": "https://bu-spark.github.io/docs/github-admin/#types-of-repositories",
    "relUrl": "/docs/github-admin/#types-of-repositories"
  },"40": {
    "doc": "Github Administration",
    "title": "Adding or removing a user to a repository",
    "content": "To add a user you need to add their Github username to the COLLABORATORS file and commit it to the correct branch(typically dev). In this case we will edit the file directly on Github so you do not have to download anything to your computer. Select dev branch . The first step is selecting the dev branch from the list of branches on Github. Create dev if it does not exist . If the dev branch does not exist you will need to create it by typing it in the box as shown below and choosing ‚ÄúCreate branch: dev from ‚Äòmain‚Äô‚Äù: . Verify current branch . Once you have created or selected the dev branch you should verify that the you are indeed on the dev branch by looking at the branch indicator on the left corner: . Locate file . Now that you are on the dev branch, locate the COLLABORATORS file in the root of the repository and select it . Edit COLLABORATORS file . Next you need to enter edit mode in order to add Github usernames, this is located on the right hand side of the screen: . Add Github usernames . Now you are editing the file and can add or remove usernames from the list. There should only be one Github username to per line. Do NOT use email addresses, this must be their github account name. Commit changes . Now that the username(s) have been added/removed we need to commit the changes to the dev branch. Scroll down and you will see a small form that you can fill out describing the changes. You only need to fill in the first text field unless further explanation is required ‚Äì good documentation is always appreciated! Verify that ‚ÄúCommit directly to dev branch‚Äù is selected and click ‚ÄúCommit Changes‚Äù button. Complete . You have now added/removed users and just need to wait for the automatic process to execute. New users will receive an email invite that will be valid for 7-days. They can click this invitation to become a collaborator on the project and be able to push code to the repository. ",
    "url": "https://bu-spark.github.io/docs/github-admin/#adding-or-removing-a-user-to-a-repository",
    "relUrl": "/docs/github-admin/#adding-or-removing-a-user-to-a-repository"
  },"41": {
    "doc": "Github Administration",
    "title": "Why not commit directly to main?",
    "content": "It might seem odd that we have made the change on dev instead of main or master. We have chosen dev because main and master are typically protected branches that do not allow anyone other than Spark! staff to make changes. This includes making edits to files such as COLLABORATORS or pushing changes from their local machines. For this reason, dev is used throughout the semester and is where students and PMs can make changes to files. ",
    "url": "https://bu-spark.github.io/docs/github-admin/#why-not-commit-directly-to-main",
    "relUrl": "/docs/github-admin/#why-not-commit-directly-to-main"
  },"42": {
    "doc": "Github Administration",
    "title": "Github Administration",
    "content": " ",
    "url": "https://bu-spark.github.io/docs/github-admin/",
    "relUrl": "/docs/github-admin/"
  },"43": {
    "doc": "Home",
    "title": "CDS Spark! Technical Resources",
    "content": "Boston University Spark! is the home for much of the experiential learning opportunies for BU students. If you want to learn more about Spark! itself, you should check out the Spark! Homepage. buspark.io is the home for resources and documentation for people engaged in a project. | You can find information about how to get resources for your project on the Requesting Compute Resources page. | If you want to know how to document your project to makje sure there is a successful handoff, check out Project Documentation Guidelines | Spark! recommends various technologies to use in your projects in the Spark! Tech Stack | Lastly, if you want to know how to interact with the automation we have in place for Github, check out GitHub Administration | . ",
    "url": "https://bu-spark.github.io/#cds-spark-technical-resources",
    "relUrl": "/#cds-spark-technical-resources"
  },"44": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "https://bu-spark.github.io/",
    "relUrl": "/"
  },"45": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "How to Present Data Science Findings to Clients ‚Äì 3 Principles and a Case Study",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#how-to-present-data-science-findings-to-clients--3-principles-and-a-case-study",
    "relUrl": "/documentation/project-guides/presentation_principles.html#how-to-present-data-science-findings-to-clients--3-principles-and-a-case-study"
  },"46": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Background",
    "content": "Students enrolled in Spark! courses/practicum are usually loaded with adequate technical skills to perform data analytics. However, there might not be a consensus how to present our findings/results to a client. Below is a blog post, originated from a conversation with a FA23 Spark! team, that could give you some insights on: How to Present Data Science Findings to Clients. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#background",
    "relUrl": "/documentation/project-guides/presentation_principles.html#background"
  },"47": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "3 Principles",
    "content": ". | Use data-driven strategies for storytellings. Support your conclusions and hypothesis with data. | Know your audience, level your presentations with your clients and explain in ways they can understand. | Embrace Ownership Principle. Task shouldn‚Äôt be limited to completing the code/presentation/report; avoid the mindset that the rest is ‚Äúnot my job‚Äù. Consider the bigger picture of how findings could benefit the client. | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#3-principles",
    "relUrl": "/documentation/project-guides/presentation_principles.html#3-principles"
  },"48": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Case Study",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#case-study",
    "relUrl": "/documentation/project-guides/presentation_principles.html#case-study"
  },"49": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Case Study Background",
    "content": "The case study used in the blog post is from a Spark! data science project that centers around unanswered survey data, where we want to help analyze survey data over the past 8-9 years. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#case-study-background",
    "relUrl": "/documentation/project-guides/presentation_principles.html#case-study-background"
  },"50": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Case Study Practice",
    "content": "Before going deeper into the blog post, imagine you were given a tabular survey data for 20 survey responders across 10 years e.g. ‚ÄúEffectiveness of head coach‚Äôs communication skills (on a scale from 1 to 4)‚Äù, and a list of the head coach changes for all the teams throughout the years. How would you use it to answer the following questions, ‚ÄúHow have athletes‚Äô perceptions of coach communication evolved over the 8-9 year period?‚Äù. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#case-study-practice",
    "relUrl": "/documentation/project-guides/presentation_principles.html#case-study-practice"
  },"51": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Two Options",
    "content": "Option A: . | A list of scatter plots of all teams throughout the 10 years | A statistical test findings over a ANOVA table and F-test to show that there is/isn‚Äôt a statistical significant difference between the years. | . Implementations: . Scatter Plot . ANOVA Table . Option B: . With the help of using an interactive dashboard such as Tableau . | A written conclusion indicating which team are under-performing/over-performing, which teams have experienced a large spike (upward and downward), whether the head coach change will impact the communications. | An interactive dashboard to show each individual team‚Äôs survey rating and head coach changing history. | . Conclusion Panel . Dashboard Panel . Which one would you choose? . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#two-options",
    "relUrl": "/documentation/project-guides/presentation_principles.html#two-options"
  },"52": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Presentation Principles Breakdown",
    "content": "If you are not sure which one to choose, please read along and reflect. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#presentation-principles-breakdown",
    "relUrl": "/documentation/project-guides/presentation_principles.html#presentation-principles-breakdown"
  },"53": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Role of a Data Scientist / Business Analyst",
    "content": "The first question that might pop into your head is: Why should I care about how I present my data findings? My model/report/analysis speaks for themselves! However, learning from my past tenure as a data scientist in marketing and product analysis, being a data scientist / business analyst is not all about deploying a model, building a dashboard. These tasks are repetitive and highly exchangeable for anyone with the particular skill set. Instead, what we really want to do is to push your client (might be your stake-holders, your manager or your product team) with your data-driven conclusions to form data-driven decisions. In that way you are part of the decision-making process and your values are magnified to a larger scale. So how do you convince your client that your idea/strategy are viable? . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#role-of-a-data-scientist--business-analyst",
    "relUrl": "/documentation/project-guides/presentation_principles.html#role-of-a-data-scientist--business-analyst"
  },"54": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Modes of Persuasion",
    "content": "In Aristotle‚Äôs Rhetoric, strategies to persuasion consist of, Ethos (an appeal to the authority or credibility of the presenter), Pathos (an appeal to the audience‚Äôs emotions), and Logos (logical appeal or the simulation of it). They are often referred as the modes of persuasion or the modes of appeal. Although ancient, it provides a great guideline on how to persuade others. After plugging in the context of data science and analytics, here are 3 most important aspects where you could apply them: . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#modes-of-persuasion",
    "relUrl": "/documentation/project-guides/presentation_principles.html#modes-of-persuasion"
  },"55": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Principle 1: Data-Driven",
    "content": "The most unique quality you possess, as a Data scientist or a Business Analyst (compared to a product manager or a software engineer),is your ability to combine storytelling and data. When doing analysis, you are often flooded with all kinds of data and metrics, and you get to be familiar with the statistical definition of the metrics. Thus, finding the correct metrics to pursue your claim is a critical step of your final deliverables. Therefore, the first principle, derived from Logos, data-driven. In the case of our case study, both options have presented data and graphs to support its conclusions. However, option A has no clear defined conclusions while option B as a clear conclusion, drawn from data ‚Äì ‚ÄúThe picking and changing of head coaches has been really successful‚Äù. The power of data-driven also lies in making actionable plans or hypothesis in the process of decision making. Suppose we are doing further correlation analysis on the survey data and you were to find directions for further improvements. Anyone can propose something or make an educated guess, what makes you different? What justifies a good idea and a bad idea? Data. In the case study, the client wants to explore the how to improve the communications between athletes and coaches. Based on your understanding, you threw out a few hypothesis: . | We could keep the head coach and let the him/her takes a communication course; | We could make an assistant position who is hired from the third party to ensure the communication goes well. | We could replace the head coach with someone with more experience. | . All of the hypothesis seems helpful, but only the third one could be supported by our survey data (See the chart below). Thus, making it more convincing in the decision-making process. While everyone else has a fascinating idea and many reasonings behind, data ensured an absolute neutral subjective way to justify decisions, thus making story-telling with data more convincing than ever. | | . | | . To further how to speak with data, the lean book series, especially Lean Analytics, is a good place to seek inspirations. The book provides many insights and frameworks that are suitable for analytics. For example, the AARRR(A) mentioned here is a common metrics framework providing guidance to many products . But is mastering the data-driven story telling enough? The answer is not yet! In many cases, your clients have a different background then yours thus leading to potential blockers in communications. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#principle-1-data-driven",
    "relUrl": "/documentation/project-guides/presentation_principles.html#principle-1-data-driven"
  },"56": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Principle 2: Know-Your-Audience",
    "content": "Compared to other technical positions such as software engineer or a machine learning engineer, data scientist and business analyst involve a lot more communication with clients from different background. Therefore, choosing the correct approach to communicate is essential, and the key to choosing the approach, derived from Pathos, is via understanding your clients and level-up with him/her. A data science venn diagram I love to use . Besides coding, statistics and, domain knowledge (business), communication is an important aspect of a data scientist‚Äôs responsibility. Imagine in a scenario, your client excels in business administration and haven‚Äôt touched statistics for a long time. Imagine the following (Also a very common data science interview questions): How would you explain p-value to your client? . Many answers pop inside your head. Citing wikipedia‚Äôs definition, ‚ÄúIn null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.‚Äù Perhaps this is not the optimal way to reciting its strict definition to your client since that will bring more jargons to the table: ‚Äútest results at least as extreme‚Äù, ‚Äúnull hypothesis‚Äù, etc. For this particular question, I would recommend checking out Cassie Kozyrkov (Google‚Äôs former Chief Decision Scientist)‚Äôs blog post where she used an excellent analogy to explain the concept: Explaining p-values with puppies. However, know your audience doesn‚Äôt simply translate to ‚Äúuse as simple words as possible‚Äù, and it‚Äôs more like ‚Äúuse the most accurate and concise words that your audience could understand‚Äù. If your audience comes from a STEM or technical background, then there is no need to re-explain basic concepts. Instead, finding the balance to communicate is what you are looking for. In the case study, an ANOVA table were presented to show that there is statistical significant correlations between coaching communication and winning across all team. However, applying the data-driven and know-your-audience principles, it could translate to: our analysis shows coaching communication has shown a strong correlation with seasonal placing, insert detailed metrics and by changing head coach to teams showing bad communication scores, we are certainly likely to, insert probability or other statistical metrics, increase our seasonal placing by a small/medium/huge advancements. Furthermore, coupled with a few presentation tricks, e.g. less is more, putting conclusion first, repeat conclusions in different ways, and hiding the details in addendum, will extremely boost the effectiveness of a client meeting. Now, you have become an excellent data scientist. But is there room improvement? The last but most important principle: . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#principle-2-know-your-audience",
    "relUrl": "/documentation/project-guides/presentation_principles.html#principle-2-know-your-audience"
  },"57": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Principle 3: Ownership",
    "content": "The word ownership is discussed at a lot of technology companies, but most commonly appeared in Amazon‚Äôs Leadership Principles. In short, one‚Äôs work shouldn‚Äôt be halted if the works goes out of the scope, using ‚Äúnot my business‚Äù as an excuse to avoid taking responsibilities. In the context of a data science project, although you spend a large amount of time in the technical pipeline, your analysis shouldn‚Äôt stop after just finishing a data visualization or a statistical analysis. A mediocre presentation and an excellent presentation has a huge difference! (While startups and VCs were at its peak a few years ago, a team with no concrete product yet a successful roadshow presentation can win over an investor easily.) . The Ownership principle could also be interpreted as ‚Äúthinking in the shoes of the clients‚Äù. In the context of the project, when the team presented their findings to its client, what will the client do with the findings? Will he/she use it to leverage change and make decisions to help the 20 teams in Boston University‚Äôs Athletics department? If so, will the findings be sufficient for he/she to convince her supervisor? her manager? Will he/she have doubts utilizing many jargons in the findings? How would he/she explain them? . If you have shown great ownership and leadership in your work, then naturally trusts are built between you and your client, thus deriving from Ethos, further communications would be easier and your words become more convincing naturally. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#principle-3-ownership",
    "relUrl": "/documentation/project-guides/presentation_principles.html#principle-3-ownership"
  },"58": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "Summary",
    "content": "In summary, presenting data science findings effectively to clients involves three key principles: . | Data-Driven Approach: Always back your conclusions with solid data. This forms the core of your credibility and persuasiveness. | Know Your Audience: Tailor your presentation to suit the background and understanding level of your clients. Avoid jargon and use clear, relatable language. | Ownership: Go beyond just presenting data. Think about how your findings can be applied in practical scenarios to benefit the client. Show initiative and consider the broader impact of your work. | . Applying these principles, as shown in the case study, will help you communicate your findings more effectively, ensuring they‚Äôre not only understood but also actionable. Remember, your role is to make data meaningful and useful for decision-making, bridging the gap between technical analysis and practical application. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html#summary",
    "relUrl": "/documentation/project-guides/presentation_principles.html#summary"
  },"59": {
    "doc": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "title": "How to Present Data Science Findings to Clients -- 3 Principles and a Case Study",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-guides/presentation_principles.html",
    "relUrl": "/documentation/project-guides/presentation_principles.html"
  },"60": {
    "doc": "Project Outline Template",
    "title": "Project Outline Template",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-document-template.html",
    "relUrl": "/documentation/project-guides/project-document-template.html"
  },"61": {
    "doc": "Project Outline Template",
    "title": "Authors,  20xx-Month-Day vx.x.x-dev",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-document-template.html#authors--20xx-month-day-vxxx-dev",
    "relUrl": "/documentation/project-guides/project-document-template.html#authors--20xx-month-day-vxxx-dev"
  },"62": {
    "doc": "Project Outline Template",
    "title": "Overview",
    "content": "A brief summary of the project based on initial research and stakeholder meetings. To the best of your abilities, explain at a high level the stakeholders‚Äô desired outcome for the project as well as the potential business value or impact this project will have if successfully completed. | Situation and current issues | Key Questions | Hypothesis: Overview of how it could be done | Impact | . A. Problem Statement: . In as direct terms as possible, provide the ‚ÄúData Science‚Äù problem statement version of the overview. Think of this as translating the above into a more technical definition to execute on. B. Checklist for project completion . Provide a bulleted list of the concrete deliverables and artifacts that, when complete, define the completion of the project. | Deliverable 1 | Deliverable 2 | . C. Provide a solution in terms of human actions to confirm if the task is within the scope of automation through AI. To assist in outlining the steps needed to achieve our final goal, outline the AI-less process that we are trying to automate with Machine Learning. Provide as much detail as possible. D. Outline a path to operationalization. Data Science Projects should have an operationalized end point in mind from the onset. Briefly describe how you see the tool produced by this project being used by the end user beyond a jupyter notebook or proof of concept. If possible, be specific and call out the relevant technologies . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-document-template.html#overview",
    "relUrl": "/documentation/project-guides/project-document-template.html#overview"
  },"63": {
    "doc": "Project Outline Template",
    "title": "Resources",
    "content": "Data Sets . | . References . | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-document-template.html#resources",
    "relUrl": "/documentation/project-guides/project-document-template.html#resources"
  },"64": {
    "doc": "Project Outline Template",
    "title": "Weekly Meeting Updates",
    "content": "Keep track of ongoing meetings in a google doc and link it to this markdown . Example meeting notes google doc. You can create a copy of this document for your project and update the meeting document as the project progresses. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-document-template.html#weekly-meeting-updates",
    "relUrl": "/documentation/project-guides/project-document-template.html#weekly-meeting-updates"
  },"65": {
    "doc": "Data Science Project Documentation Guidelines",
    "title": "Data Science Project Documentation Guidelines",
    "content": "Useful Resources . | Project Workflow Guide: The purpose of this document is to provide data science project groups with some guidelines on how to best structure their projects in a way that encourages consistent workflows across projects and promotes collaboration. | Project Document Template: This document serves as a template for a project outline documentation. It includes project goals, plans for implementation, proposed methologies and links to meeting notes documents. Teams can use this template to structure their project outlines, basing it off their meeting with stakeholders, project documents provided to them. | Exploratory Data Analysis: This document gives an introduction to exploratory data analysis. It includes descriptions on popular EDA python packages, different types of data and visualizations, and commonly-used statistical measures used in EDA. | EDA Example: This document provides an example of what a complete EDA looks like. See one way to clean, analyze, and explore a Boston 311-service request dataset in python using Pandas, Numpy, MatplotLib, Seaborn, and a geocoder for spatial data. | . ",
    "url": "https://bu-spark.github.io/docs/project-guides/",
    "relUrl": "/docs/project-guides/"
  },"66": {
    "doc": "Spark! Project Organization",
    "title": "Introduction",
    "content": "Spark! provides a number of tools to support project teams to be as effective as possible. You can find the the conventions and tools we recommend here. Many of our recommendations are based on common usage in industry not necessarily the best in class of that tool. ",
    "url": "https://bu-spark.github.io/docs/project-organization/#introduction",
    "relUrl": "/docs/project-organization/#introduction"
  },"67": {
    "doc": "Spark! Project Organization",
    "title": "Tools",
    "content": ". | Slack | . ",
    "url": "https://bu-spark.github.io/docs/project-organization/#tools",
    "relUrl": "/docs/project-organization/#tools"
  },"68": {
    "doc": "Spark! Project Organization",
    "title": "Spark! Project Organization",
    "content": " ",
    "url": "https://bu-spark.github.io/docs/project-organization/",
    "relUrl": "/docs/project-organization/"
  },"69": {
    "doc": "Project Workflow Guide",
    "title": "How to Structure a Data Science Project",
    "content": "The purpose of this document is to provide data science project groups with some guidelines on how to best structure their projects in a way that encourages consistent workflows across projects and promotes collaboration. This document is based off of Red Hat‚Äôs ET team‚Äôs data science workflows documentation. Following is a timeline that can be followed for the projects. ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-structure.html#how-to-structure-a-data-science-project",
    "relUrl": "/documentation/project-guides/project-structure.html#how-to-structure-a-data-science-project"
  },"70": {
    "doc": "Project Workflow Guide",
    "title": "How do I manage my project?",
    "content": "Organizational overhead can be an easily overlooked, and a potentially crippling oversight for a new data science project. To make things worse, data science is still a relatively new discipline and the right tools and methodologies for project management are still being figured out. The approach to overcoming this issue can simply be to focus on the fewest number of tools possible. That really comes down to using version control for your repositories, project management boards to organize tasks, and a communication method for real time conversations. What do you need to start your project and keep it moving? . | Project Repository . | Public repository (examples) | Helpful bots | . | You can also jump start your repo with project templates eg. cookiecutter, or an adaption of it here . | Project Board . | Github project board (examples) | Github issue templates (examples) | Trello Kanban template | . | Communication Channel . | Project channel(s) (example) | . | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-structure.html#how-do-i-manage-my-project",
    "relUrl": "/documentation/project-guides/project-structure.html#how-do-i-manage-my-project"
  },"71": {
    "doc": "Project Workflow Guide",
    "title": "What are the phases of a data science project?",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-structure.html#what-are-the-phases-of-a-data-science-project",
    "relUrl": "/documentation/project-guides/project-structure.html#what-are-the-phases-of-a-data-science-project"
  },"72": {
    "doc": "Project Workflow Guide",
    "title": "Phase 1: Project Definition and Use Case Understanding:",
    "content": "In this phase of the project, project groups will meet with Stakeholders to discuss the projects, available data and desired outcomes, use cases and end-users. | To start, project groups should create a repository and include a README with an initial outline of the project. | With the help of the information provided by the stakeholders, project groups come up with a ‚Äúproject outline‚Äù consisting of the description of any available data or similar projects and the desired outcomes and end-users of this intelligent application. Please feel free to start with this template for inspiration. | . The project outline can include: . | A concrete checklist of items or deliverables that define when the project is complete (subject to ongoing review). | Defined performance criteria and success metrics that will be used to evaluate the success of the machine learning models. | An alternative solution written in terms as though it would be solved manually by a human without AI, i.e, ‚ÄúI use my eyes to look at a picture and respond that it is or is not a cat based on my knowledge of the appearance of cats‚Äù. This helps to identify any potential mismatch between desired outcome and available data. | Outline a path to operationalization. Make sure there is some consideration given to the infrastructure, deployment, and user interactions of your projects final form. | . Once this is done, you can move on to step 2, OR share your new project proposal with your stakeholders, instructors, to get some feedback. Deliverables from Phase 1: . | Project Github Repository following a standard template with updated Readme. | Project Outline markdown added to repository using the project outline template. | Midterm Presentation Slides on plan to execute project | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-structure.html#phase-1-project-definition-and-use-case-understanding",
    "relUrl": "/documentation/project-guides/project-structure.html#phase-1-project-definition-and-use-case-understanding"
  },"73": {
    "doc": "Project Workflow Guide",
    "title": "Phase 2: Research and Problem Understanding:",
    "content": "Now that you have a problem you‚Äôd like to solve, its time to do some research! This phase of a project is quite flexible and is bound to be traversed differently by every individual. However, before jumping into any hard-core development its is generally good to make sure to do the following at the bare minimum: . | Compile and read 3-4 relevant papers on the problem domain. Papers With Code and arxiv will be your friends here. | Find 1-2 comparable open source projects and try to deploy them yourself. Maybe the solution you‚Äôre looking for already exists and you can re-use or contribute to an existing project instead of starting from scratch. | Add a research.md doc to your repository outlining the papers and projects explored above. This may be useful to you for reference or documentation later on. Plus, anyone who needs to do the same research in the future will thank you. | . At this phase in the project, teams can present a modified plan for achieving project goal and share with stakeholders for general agreement. This can include: . | Data Requirements | Performance Criteria | ML approach and reasoning | Deliverable Artifacts | . Deliverables from Phase 2: . | Research document markdown added to the project repository outlining possible approaches. | Update checklist for project completion section with any new discoveries. | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-structure.html#phase-2-research-and-problem-understanding",
    "relUrl": "/documentation/project-guides/project-structure.html#phase-2-research-and-problem-understanding"
  },"74": {
    "doc": "Project Workflow Guide",
    "title": "Phase 3: Data Preparation and EDA:",
    "content": "Time to dig into the data. Before getting right into the Machine Learning development, we think it‚Äôs best practice to take some focused time to familiarize yourself with the data and to write all the data preparation, processing and management functions you‚Äôll need to make life easier in the future. | Work with stakeholders to ensure access to data. Determine the nature of data and store the data appropriately depending on the nature of data (eg: open source, sensitivity, volume). | Make sure the data being referenced in code remains consistent and is available and accessible by all contributors. It is important to maintain versions of datasets and store intermediate, processed datasets also along with raw datasets. | You can consider using a public/private S3 bucket to store and retrieve large amounts of data. For open source data for requesting a public bucket on the operate-first cluster, you can request one here by opening an issue. Another way to store the data can be directly on the Github repository. Discuss the data storage method with your stakeholders and make sure that there is consensus. | Work with stakeholders to create description of data/metadata. Determine if we need SME labeled or augmented data at this stage. Think of ways to explore the initial data provided before generating augmented data or looking for alternate data sources. | Develop an EDA Notebook to help explain to others (and your future self) what the data looks like and what it represents. | Write all necessary ETL, data cleaning and preprocessing code. | Select and separate immutable training, testing and evaluation datasets. Store each separately in your storage system. | Make any required updates to the project Readme/plan if needed due to discoveries in the data set. | . Deliverables from Phase 3: . | Exploratory Data Analysis (EDA) Notebook added to Project Repository. | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-structure.html#phase-3-data-preparation-and-eda",
    "relUrl": "/documentation/project-guides/project-structure.html#phase-3-data-preparation-and-eda"
  },"75": {
    "doc": "Project Workflow Guide",
    "title": "Phase 4: Implement proof of concept AI/ML model:",
    "content": "Here is were we start to do some ML exploration and put together our bare-bones MVP (minimum viabl) application. | Create a jupyter notebook and start experimenting with different models and approaches until your results meet the performance criteria defined earlier. At this stage there are a number of options for experiment tracking, eg: ml flow, kubeflow pipelines. | Clean up your experimental notebook so that another group member or data scientist would be able to follow the logic, re-run it and understand your results. This means, clearly written markdown cells, well labeled plots and commented code. | Once your Proof of Concept notebook(s) are done. Push them to your github repo. | You can also share your code and documentation as a Jupyterbook using project meteor . | . Note: Focus on completing an end to end process of deploying an initial version of the model which meets the required criteria rather than focusing on achieving the best possible model for the first deployment. Greater focus should be on creating reproducible code and models and services that are set up in a way that they be improved in future iterations with more feedback. Often deploying a working proof of concept model and sharing that with the stakeholders early on in a project, will help create dialogue, fine tune requirements, and can generate good feedback early in the process of a building a machine learning project. By adopting a software engineering mindset and engineering best practices for machine learning projects, we can create more reproducible code, faster delivery cycles and better machine learning applications. Deliverables from Phase 4: . | Proof of Concept (PoC) Notebook added to Project Repository. | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-structure.html#phase-4-implement-proof-of-concept-aiml-model",
    "relUrl": "/documentation/project-guides/project-structure.html#phase-4-implement-proof-of-concept-aiml-model"
  },"76": {
    "doc": "Project Workflow Guide",
    "title": "Phase 5: Deployed PoC as a service:",
    "content": "Your work is ready to leave the safety of a jupyter notebook and become a deployed service that will become the center of a new intelligent application. Get Excited! . | Deploy your model. There are a number of tools out there to make this easy. You can choose to deploy your model as a web service, as a containerized application. (example: Seldon serving on Openshift for model deployment) . | Write a short notebook that can send and receive inference requests from your model services endpoint to make it easy for others to test and try out. This will serve as an enpoint for stakeholders to try out the model. So focus on making their interaction with the model easy. | . Deliverables from Phase 5: . | Deployed model available as a service. | Instructions to interact with model or example code to run model. | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-structure.html#phase-5-deployed-poc-as-a-service",
    "relUrl": "/documentation/project-guides/project-structure.html#phase-5-deployed-poc-as-a-service"
  },"77": {
    "doc": "Project Workflow Guide",
    "title": "Phase 6. Evaluation:",
    "content": "Now that your model is out in the wild, its a good idea to see if it‚Äôs actually useful to anyone and evaluate its real world performance. | Share your work on communication channels, or present it at to get people using and testing out your model. | If there are any issues or areas to improve (which there will be) loop back to previous steps in this workflow and repeat until your project meets your desired outcome. | Monitor your model deployed in production for its performance on actual data, data drift and consider approaches to re-train and continuously improve the model. | . Deliverables from Phase 6: . | Presentation Slides on deployed application. | Monitoring plan for the application. | . ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-structure.html#phase-6-evaluation",
    "relUrl": "/documentation/project-guides/project-structure.html#phase-6-evaluation"
  },"78": {
    "doc": "Project Workflow Guide",
    "title": "Project Workflow Guide",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-guides/project-structure.html",
    "relUrl": "/documentation/project-guides/project-structure.html"
  },"79": {
    "doc": "Requesting Compute Resources",
    "title": "Compute Resources",
    "content": "Spark! has several compute resources available for student projects. Student‚Äôs should not be creating hosted resources on their own personal accounts(AWS, GCP etc) instead they should use the Spark! accounts unless specifically told otherwise by Spark! staff. ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html#compute-resources",
    "relUrl": "/documentation/compute-resources/request-resources.html#compute-resources"
  },"80": {
    "doc": "Requesting Compute Resources",
    "title": "Resources Offered",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html#resources-offered",
    "relUrl": "/documentation/compute-resources/request-resources.html#resources-offered"
  },"81": {
    "doc": "Requesting Compute Resources",
    "title": "Google Cloud Platform(GCP)",
    "content": "We can help you get setup with a paid account on GCP to leverage any service offered. ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html#google-cloud-platformgcp",
    "relUrl": "/documentation/compute-resources/request-resources.html#google-cloud-platformgcp"
  },"82": {
    "doc": "Requesting Compute Resources",
    "title": "AWS Resources",
    "content": "We do not offer AWS services in general. If a comparable resource is not offered by Google Cloud Platform then we permit it on a one off basis. ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html#aws-resources",
    "relUrl": "/documentation/compute-resources/request-resources.html#aws-resources"
  },"83": {
    "doc": "Requesting Compute Resources",
    "title": "Github Pages",
    "content": "If you just need to deploy your static website Github pages is the simplest option. ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html#github-pages",
    "relUrl": "/documentation/compute-resources/request-resources.html#github-pages"
  },"84": {
    "doc": "Requesting Compute Resources",
    "title": "Shared Computing Cluster (SCC)",
    "content": "The SCC is a service offered by BU that gives you access to a GPUs and CPUs for running long running tasks such as machine learning or other data intensive projects. Want to know more about the SCC? Check out their site and their docs. For Data Science and Machine Learning related projects, this is the preferred environment. ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html#shared-computing-cluster-scc",
    "relUrl": "/documentation/compute-resources/request-resources.html#shared-computing-cluster-scc"
  },"85": {
    "doc": "Requesting Compute Resources",
    "title": "Website Hosting",
    "content": "The need to host your website is common and needed by almost all projects. We have a few different ways in which we can support you in this endeavor. Below is a list of options, requesting access follows the same procedures as all other resources. | Google Firebase ‚Äì Offers static site hosting and serverless functions | Cloudflare ‚Äì Offers static site hosting and serverless functions | Google Cloud Run ‚Äì Hosting for both static and dynamic websites. | . ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html#website-hosting",
    "relUrl": "/documentation/compute-resources/request-resources.html#website-hosting"
  },"86": {
    "doc": "Requesting Compute Resources",
    "title": "Backend (API) Hosting",
    "content": "If you need to host your API for a Spark! project you can and should do so on a Spark! owned account. Below is a list of options for hosting your API. | Google Cloud Platform ‚Äì GCP offers everything needed to host an API. Including severless, containzered and traditional APIs. | Something else ‚Äì In special cases we will approve something else. Reach out if you don‚Äôt think your project will work on one of the other options. | . ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html#backend-api-hosting",
    "relUrl": "/documentation/compute-resources/request-resources.html#backend-api-hosting"
  },"87": {
    "doc": "Requesting Compute Resources",
    "title": "Other",
    "content": "We do have some other random resources. If you don‚Äôt see what you need, reach out to us and we can help you out! . ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html#other",
    "relUrl": "/documentation/compute-resources/request-resources.html#other"
  },"88": {
    "doc": "Requesting Compute Resources",
    "title": "How to request access",
    "content": "In order to request access to resources please do the following: . | Open an issue on your project‚Äôs Github Repository with the following structure: . Title: . Request for Spark! Tech Resources for [your project name] . Content: . - Names + emails of all team members who need access - Detailed list of resources needed - A complete description on how you plan to use the resources - List any relevant course deadlines . | Assign the appropriate person to the issue: . | For Software Engineering related projects . | Assign the created issue to @IanSaucy on Github | Send an email to with a link to the Github Issue. You should copy both your PM and course instructor on this email as well. This email is required. | . | For Data Science or Machine Learning related projects . | Assign the created issue to @mvoong18 on Github | Send an email to with a link to the Github Issue. You should copy both your PM and course instructor on this email as well. This email is required. | . | . | Future correspondence should happen through the Github Issue | If you don‚Äôt hear an update within 4 days, bump on Github or via email. | . ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html#how-to-request-access",
    "relUrl": "/documentation/compute-resources/request-resources.html#how-to-request-access"
  },"89": {
    "doc": "Requesting Compute Resources",
    "title": "Requesting Compute Resources",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/compute-resources/request-resources.html",
    "relUrl": "/documentation/compute-resources/request-resources.html"
  },"90": {
    "doc": "Slack",
    "title": "Slack Channel Naming",
    "content": "Slack channels are used for different purposes and the name of those channels should consistently reflect the usage for that channel. As a result, we have adopted the following convention: . [i-]&lt;season&gt;&lt;2 digit year&gt;-[type]-&lt;project&gt;-[mgmt/inst] . ",
    "url": "https://bu-spark.github.io/documentation/project-organization/slack.html#slack-channel-naming",
    "relUrl": "/documentation/project-organization/slack.html#slack-channel-naming"
  },"91": {
    "doc": "Slack",
    "title": "Explanation",
    "content": ". | angle brackets (&lt;&gt;) mean the information needs to be fillled in but is required | square brackets ([]) mean that the information is optional but is literal | [i-] ‚Äì indictates that the channel is internal and only the project team no third parties (like partners) | &lt;season&gt; ‚Äì one of sp, fa, sum to indicate when the project is taking place | &lt;2 digit year&gt; ‚Äì the year in two digits | [type] ‚Äì This is an optional field but should be used if the project does have a type. Possible options are, ds(Data Science), se(Software Engineering), ml(Machine Learning), ux (UI/UX), and xcc(CoLabs/XCCs). | &lt;project&gt; ‚Äì name of the project or class, preferably as short as reasonable | [mgmt] ‚Äì optional, should include only Spark! staff, PMs, and TEs. May include EIRs and Mentors | [inst] ‚Äì optional, should include only instructors and, at the instructor‚Äôs discretion, others | . ",
    "url": "https://bu-spark.github.io/documentation/project-organization/slack.html#explanation",
    "relUrl": "/documentation/project-organization/slack.html#explanation"
  },"92": {
    "doc": "Slack",
    "title": "Examples:",
    "content": ". | i-fa22-myproj | i-fa22-myproj-mgmt | fa22-myproj | i-sp22-myproj-inst | . ",
    "url": "https://bu-spark.github.io/documentation/project-organization/slack.html#examples",
    "relUrl": "/documentation/project-organization/slack.html#examples"
  },"93": {
    "doc": "Slack",
    "title": "Slack",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/project-organization/slack.html",
    "relUrl": "/documentation/project-organization/slack.html"
  },"94": {
    "doc": "Software Engineering Tech Stack",
    "title": "Universal Features",
    "content": ". | All projects should ideally be configured with the following solutions: | Each project should have a linter configured and applied at commit and or on push to github Automatic build and deploy(CI/CD) on push to production branches | . ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/software-engineering.html#universal-features",
    "relUrl": "/documentation/tech-stack/software-engineering.html#universal-features"
  },"95": {
    "doc": "Software Engineering Tech Stack",
    "title": "Front End Development",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/software-engineering.html#front-end-development",
    "relUrl": "/documentation/tech-stack/software-engineering.html#front-end-development"
  },"96": {
    "doc": "Software Engineering Tech Stack",
    "title": "React.js + Typescript",
    "content": "React.JS, commonly referred to as React, is an open-source library based on JavaScript and JSX (a PHP extension). It was released in 2013 by Facebook to create flexible, dynamic user interfaces. Two of React‚Äôs most useful features are: . | use of the Virtual DOM (Document Object Model), which facilitates the creation of fast and responsive UIs while maintaining high app performance; | component-based architecture, which is easier to maintain than other architectures. | . TypeScript has gained immense popularity among frontend developers the past few years due to improved maintainability, code consistency, type checking, and future browser support. This trend can also be seen in the industry as well. React gives developers the freedom to choose between Typescript and Javascript. Typescript can natively be integrated with React. Strict typing allows for easy debugging and cleaner code for complex projects. Code also becomes more self descriptive because the types‚Äô definitions also work as documentation. As a result, when you come back to the code after a while it‚Äôs easier to remember what it does by just reading it. However Typescript has a slightly higher learning curve - currently not taught by any CS courses right now in BU. Leveraging the Create React App(CRA) Typescript setup is easy to add to existing or new React.js projects. Getting Started . Create React App(CRA) is the easiest way to get started with React.js. | Getting Started with Create React App . | CRA with Typescript . | . Resources: . | https://programmingwithmosh.com/javascript/react-typescript/ | https://insights.stackoverflow.com/survey/2020#most-popular-technologies | https://insights.stackoverflow.com/survey/2020 | . ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/software-engineering.html#reactjs--typescript",
    "relUrl": "/documentation/tech-stack/software-engineering.html#reactjs--typescript"
  },"97": {
    "doc": "Software Engineering Tech Stack",
    "title": "Mobile Applications",
    "content": "There are inherently less options for mobile applications. Either projects are built natively in which case, only a single option exists for each platform. Other projects would work well with a cross-platform development framework and should use one of the options below. Backend choices can be chosen from the backend section below. ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/software-engineering.html#mobile-applications",
    "relUrl": "/documentation/tech-stack/software-engineering.html#mobile-applications"
  },"98": {
    "doc": "Software Engineering Tech Stack",
    "title": "Flutter - Cross-platform Mobile App Dev",
    "content": "Flutter is an open-source portable UI toolkit built by Google, it‚Äôs a comprehensive app Software Development Kit (SDK), complete with widgets and tools. Flutter enables cross-platform app development. It gives developers an easy way to build and deploy visually attractive, natively-compiled applications for mobile (iOS, Android), web, and desktop ‚Äì all using a single codebase. Features: . | It‚Äôs based on Dart ‚Äì a fast, object-oriented programming language. It‚Äôs relatively new and easy to learn ‚Äì especially for experienced developers more familiar with Java and C#. | The architecture is based on the very popular reactive programming (it follows the same style as React) | It provides its own widgets(similar to components in React), drawn from its own high-performance rendering engine ‚Äì these are fast, attractive and customizable | Excellent documentation with strong support from the Flutter team | Very fast development time, good developer experience | . Getting Started . Google provides excelent documentation and can be leveraged as your main source for documentation. | Getting Started with React | . Flutter Add-ons . This is a non-comprehensive list of extensions and add-ons to the core flutter framework that Spark! recommends. State Management . This is an inherently opinionated and project driven choice but we recommend the following: . | Flutter RiverPod - Most modern and fully featured. Stepper learning curve | Provider - Less features but easier to learn, superseded by RiverPod | . ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/software-engineering.html#flutter---cross-platform-mobile-app-dev",
    "relUrl": "/documentation/tech-stack/software-engineering.html#flutter---cross-platform-mobile-app-dev"
  },"99": {
    "doc": "Software Engineering Tech Stack",
    "title": "Backend",
    "content": "The backend of each project can be built using one of the combinations below. Severless provides the fastest time to run but does not enable as much customization as other options. ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/software-engineering.html#backend",
    "relUrl": "/documentation/tech-stack/software-engineering.html#backend"
  },"100": {
    "doc": "Software Engineering Tech Stack",
    "title": "Google Firebase",
    "content": "Firebase provides a pre-configured backend hosted and run by Google. Easy to learn and use. ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/software-engineering.html#google-firebase",
    "relUrl": "/documentation/tech-stack/software-engineering.html#google-firebase"
  },"101": {
    "doc": "Software Engineering Tech Stack",
    "title": "Python Based API",
    "content": "Build a Python based API using: . | Flask, or FastAPI | JWT or OAuth for all authentication | Hosted SQL Database | All running inside containers | . ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/software-engineering.html#python-based-api",
    "relUrl": "/documentation/tech-stack/software-engineering.html#python-based-api"
  },"102": {
    "doc": "Software Engineering Tech Stack",
    "title": "Node.js Based API",
    "content": "Node.js API using the following technologies . | Node.js + Express | JWT or OAuth for authenticated requests | Hosted SQL Database All running inside containers | . ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/software-engineering.html#nodejs-based-api",
    "relUrl": "/documentation/tech-stack/software-engineering.html#nodejs-based-api"
  },"103": {
    "doc": "Software Engineering Tech Stack",
    "title": "Software Engineering Tech Stack",
    "content": " ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/software-engineering.html",
    "relUrl": "/documentation/tech-stack/software-engineering.html"
  },"104": {
    "doc": "Spark! Tech Stack",
    "title": "Introduction",
    "content": "Spark! has selected a set of technology stacks(tech stacks) and development suggestions with the end goal of having better knowledge transfer between different projects within Spark!. Although a selection of a tech stack from this document is recommended, student teams are free to select a different combination of technologies. Students are encouraged to select one of the preferred tech stacks as there are more resources and assistance available to students through Spark! when using one of the preferred tech stacks. ",
    "url": "https://bu-spark.github.io/docs/tech-stack/#introduction",
    "relUrl": "/docs/tech-stack/#introduction"
  },"105": {
    "doc": "Spark! Tech Stack",
    "title": "Specific Tech stacks",
    "content": "We have a few different documents depending on the type of project you are working on. See the links below for specifics. | Software Engineering Tech Stack | UI/UX Design Tech Stack | Data Science Tech Stack | . ",
    "url": "https://bu-spark.github.io/docs/tech-stack/#specific-tech-stacks",
    "relUrl": "/docs/tech-stack/#specific-tech-stacks"
  },"106": {
    "doc": "Spark! Tech Stack",
    "title": "Spark! Tech Stack",
    "content": " ",
    "url": "https://bu-spark.github.io/docs/tech-stack/",
    "relUrl": "/docs/tech-stack/"
  },"107": {
    "doc": "UI/UX Tech Stack",
    "title": "UI/UX Tech Stack",
    "content": "We have selected two different tools that we wish all projects to utilize. This is not a set list, if you think another tool would better fit your project, allow you better creativity etc then please reach out to your respective staff advisor and propose this. We want to work with you ‚Äì but if you don‚Äôt have a strong preference we will ask that you leverage our standard options. ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/ui-ux.html",
    "relUrl": "/documentation/tech-stack/ui-ux.html"
  },"108": {
    "doc": "UI/UX Tech Stack",
    "title": "Design Choice",
    "content": "We have selected Figma as our choice of design software. We have an education account that gives us access to the professional version for more features etc. All your design files should be within a project that is owned by BU Spark!. As the UI/UX designer you should receive an invite to the project where you can access, create and edit existing work. At no point should you create and store designs on your personal Figma account. ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/ui-ux.html#design-choice",
    "relUrl": "/documentation/tech-stack/ui-ux.html#design-choice"
  },"109": {
    "doc": "UI/UX Tech Stack",
    "title": "Handoff Software",
    "content": "We have selected Zeplin.io as a piece of software to help handoff the designs to a developer team. This is an important part of the process and we have found Zeplin to help ease the transition. It gives you, the UI/UX designer more control over which design is shared with the project partner, software developer etc. It also gives developers access to layouts in a manner that is more amendable for building a website, app etc. If you‚Äôd like to use Zeplin for your project please contact Ian on Slack. ",
    "url": "https://bu-spark.github.io/documentation/tech-stack/ui-ux.html#handoff-software",
    "relUrl": "/documentation/tech-stack/ui-ux.html#handoff-software"
  }
}
